# data.py
from __future__ import annotations

import json
import re
import datetime
import textwrap
import pprint
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import yaml

# ----------------------------
# Swift L10n.swift ç”Ÿæˆ
# ----------------------------


def _swift_escape(s: str) -> str:
    """å°†æ–‡æœ¬è½¬ä¹‰ä¸º Swift å­—ç¬¦ä¸²å­—é¢é‡å¯ç”¨çš„å½¢å¼ã€‚"""
    if s is None:
        return ""
    # é¡ºåºå¾ˆé‡è¦ï¼šå…ˆè½¬ä¹‰åæ–œæ 
    s = s.replace("\\", "\\\\")
    s = s.replace('"', "\\\"")
    s = s.replace("\r\n", "\\n").replace("\n", "\\n").replace("\r", "\\n")
    return s


_COMMENT_STRIP_RE = re.compile(r"^\s*(?://+|/\*+|\*+|\*/+)\s*|\s*(?:\*/)?\s*$")


def _comment_to_doc_line(comments: List[str]) -> Optional[str]:
    """æŠŠ .strings ä¸Šæ–¹çš„æ³¨é‡Šå—æç‚¼æˆä¸€è¡Œ /// æ–‡æ¡£æ³¨é‡Šï¼ˆå°½é‡è´´è¿‘åŸæ–‡ä»¶ï¼‰ã€‚"""
    if not comments:
        return None
    # æ‰¾â€œæœ€åä¸€è¡Œæœ‰å†…å®¹â€çš„æ³¨é‡Šï¼ˆé€šå¸¸æœ€è´´è¿‘ key çš„è¯­ä¹‰ï¼‰
    for raw in reversed(comments):
        s = raw.strip()
        if not s:
            continue
        if not _is_comment_line(s):
            # éæ ‡å‡†è¡Œä¹Ÿå…è®¸ï¼ˆparse_strings_file å·²ç»æŠŠå®ƒå½’åˆ° comments é‡Œï¼‰
            pass
        # å»æ‰ // /* * */ ç­‰æ ‡è®°
        s = _COMMENT_STRIP_RE.sub("", s).strip()
        if s:
            return s
    return None


def _to_pascal_case(s: str) -> str:
    # ä»…åšæœ€å°è§„åˆ™ï¼šæŒ‰ '_' åˆ†è¯ï¼Œé¦–å­—æ¯å¤§å†™ï¼Œå…¶ä½™åŸæ ·ä¿ç•™ï¼ˆå…¼å®¹ historyLocations è¿™ç§ camelï¼‰
    parts = [p for p in re.split(r"[_\s]+", s) if p]
    if not parts:
        return "X"
    return "".join(p[:1].upper() + p[1:] for p in parts)


def _to_camel_case_from_key_remainder(rem: str) -> str:
    """æŠŠ group ä¹‹åçš„ key remainderï¼ˆå¯èƒ½å« '.'/'_'ï¼‰è½¬æˆ lowerCamelCase å±æ€§åã€‚"""
    # ä»¥ '.' åˆ†æ®µï¼›æ¯æ®µå†ä»¥ '_' åˆ†è¯
    segs: List[str] = []
    for seg in rem.split("."):
        seg = seg.strip()
        if not seg:
            continue
        segs.extend([w for w in seg.split("_") if w])

    if not segs:
        return "value"

    out = segs[0]
    for w in segs[1:]:
        out += w[:1].upper() + w[1:]
    # Swift æ ‡è¯†ç¬¦ä¸èƒ½ä»¥æ•°å­—å¼€å¤´ï¼›æç«¯æƒ…å†µå…œåº•
    if out and out[0].isdigit():
        out = "_" + out
    return out


def _swift_prop_name_for_key(key: str) -> Tuple[str, str]:
    """
    âœ… ä¸ generate_l10n_swift å®Œå…¨ä¸€è‡´çš„å±æ€§åæ¨å¯¼é€»è¾‘ï¼Œä½†è¿”å› (group_prefix, prop)
    - grp = _group_prefix(key)  -> Swift enum åæ¥æº
    - rem = å»æ‰ grp + '.' æˆ– grp + '_'ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    - prop = _to_camel_case_from_key_remainder(rem) -> enum å†… static var å
    """
    grp = _group_prefix(key)
    rem = key
    if "." in key and key.startswith(grp + "."):
        rem = key[len(grp) + 1 :]
    elif "_" in key and key.startswith(grp + "_"):
        rem = key[len(grp) + 1 :]
    prop = _to_camel_case_from_key_remainder(rem)
    return grp, prop



def scan_camelcase_conflicts(entries: List["StringsEntry"]) -> Dict[str, List[str]]:
    """
    æ‰«æâ€œé©¼å³°åŒ–ååŒåï¼Œä½†åŸ key ä¸åŒâ€çš„å†²çªï¼ˆSwift L10n ç”Ÿæˆä¼šæ’å±æ€§åï¼‰ã€‚
    è¿”å›ï¼š{prop_name: [key1, key2, ...]}ï¼ˆä»…ä¿ç•™å†²çªé¡¹ï¼Œä¸” key åˆ—è¡¨æ’åºå»é‡ï¼‰
    """
    bucket: Dict[str, List[str]] = {}
    for e in entries:
        prop = _swift_prop_name_for_key(e.key)
        bucket.setdefault(prop, []).append(e.key)

    out: Dict[str, List[str]] = {}
    for prop, keys in bucket.items():
        uniq = sorted(set(keys))
        if len(uniq) >= 2:
            out[prop] = uniq
    return out


def _format_camel_conflicts(conflicts: Dict[str, List[str]], *, header: str) -> str:
    lines: List[str] = []
    lines.append(header)
    for prop in sorted(conflicts.keys()):
        keys = conflicts[prop]
        lines.append(f"- {prop}: {keys}")
    return "\n".join(lines)


def generate_l10n_swift(
    cfg: "StringsI18nConfig",
    *,
    strings_filename: str = "Localizable.strings",
    out_path: Optional[Path] = None,
) -> Path:
    """ä» Base.lproj/<strings_filename> ç”Ÿæˆ L10n.swiftï¼ˆæŒ‰ key å‰ç¼€åˆ†ç»„ï¼‰ã€‚"""
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    src_fp = (base_dir / strings_filename).resolve()
    if not src_fp.exists():
        raise FileNotFoundError(f"æœªæ‰¾åˆ° Base strings æ–‡ä»¶ï¼š{src_fp}")

    # âœ… äº§ç‰©çº¦å®šï¼šL10n.swift æ”¾åœ¨ lang_root ä¸‹é¢
    # - out_path ä¸ºç©ºï¼šé»˜è®¤ <lang_root>/L10n.swift
    # - out_path ä¸ºç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹ <lang_root>
    # - out_path ä¸ºç»å¯¹è·¯å¾„ï¼šæŒ‰ç»å¯¹è·¯å¾„å†™
    if out_path is None:
        out_path = (cfg.lang_root / "L10n.swift")
    elif not out_path.is_absolute():
        out_path = (cfg.lang_root / out_path)
    out_path = out_path.resolve()
    out_path.parent.mkdir(parents=True, exist_ok=True)

    _preamble, entries = parse_strings_file(src_fp)
    # ç¨³å®šæ’åºï¼šå…ˆåˆ†ç»„å†æŒ‰ key
    entries = sorted(entries, key=lambda e: (_group_prefix(e.key), e.key))

    # âœ… ç”Ÿæˆå‰çš„é˜²çˆ†ï¼šBase å†… camelCase å†²çªç›´æ¥æŠ¥é”™ï¼ˆå¦åˆ™ Swift ç¼–è¯‘ç‚¸ï¼‰
    camel_conflicts = scan_camelcase_conflicts(entries)
    if camel_conflicts:
        msg = _format_camel_conflicts(
            camel_conflicts,
            header=f"Base/{strings_filename} å­˜åœ¨ Swift camelCase å±æ€§åå†²çªï¼ˆè¯·å…ˆæ‰‹åŠ¨æ”¹ keyï¼‰ï¼š",
        )
        raise ValueError(msg)

    # æŒ‰ group_prefix èšåˆ
    groups: Dict[str, List[StringsEntry]] = {}
    for e in entries:
        groups.setdefault(_group_prefix(e.key), []).append(e)

    lines: List[str] = []
    lines.append(f"// Auto-generated from {cfg.base_folder}/{strings_filename}")
    lines.append("import Foundation")
    lines.append("")
    lines.append("extension String {")
    lines.append("    func callAsFunction(_ arguments: CVarArg...) -> String {")
    lines.append("        String(format: self, locale: Locale.current, arguments: arguments)")
    lines.append("    }")
    lines.append("}")
    lines.append("")
    lines.append("enum L10n {")

    for grp in sorted(groups.keys(), key=lambda x: x.lower()):
        enum_name = _to_pascal_case(grp)
        lines.append(f"    enum {enum_name} {{")

        for e in groups[grp]:
            # remainderï¼šå»æ‰ group_prefix + åˆ†éš”ç¬¦
            rem = e.key
            if "." in e.key and e.key.startswith(grp + "."):
                rem = e.key[len(grp) + 1 :]
            elif "_" in e.key and e.key.startswith(grp + "_"):
                rem = e.key[len(grp) + 1 :]

            prop = _to_camel_case_from_key_remainder(rem)
            doc = _comment_to_doc_line(e.comments)
            if doc:
                lines.append(f"        /// {doc}")

            key_esc = _swift_escape(e.key)
            val_esc = _swift_escape(e.value)
            cmt_esc = _swift_escape(e.value)  # ä¸ç°æœ‰æ ·ä¾‹ä¸€è‡´ï¼šcomment ä½¿ç”¨åŒæ–‡æ¡ˆ

            lines.append(
                f"        static var {prop}: String {{ return NSLocalizedString(\"{key_esc}\", value: \"{val_esc}\", comment: \"{cmt_esc}\") }}"
            )
            lines.append("")

        # å»æ‰ enum å†…æœ«å°¾å¤šä½™ç©ºè¡Œ
        while lines and lines[-1] == "":
            lines.pop()
        lines.append("    }")
        lines.append("")

    # å»æ‰æ–‡ä»¶æœ«å°¾å¤šä½™ç©ºè¡Œ
    while lines and lines[-1] == "":
        lines.pop()
    lines.append("}")
    lines.append("")

    out_path.write_text("\n".join(lines), encoding="utf-8")
    return out_path


# ----------------------------
# å¸¸é‡ / é»˜è®¤æ–‡ä»¶å
# ----------------------------
DEFAULT_TEMPLATE_NAME = "strings_i18n.yaml"     # å†…ç½®æ¨¡æ¿æ–‡ä»¶ï¼ˆå¸¦æ³¨é‡Šï¼‰
DEFAULT_LANGUAGES_NAME = "languages.json"      # æœ¬åœ°è¯­è¨€åˆ—è¡¨æ–‡ä»¶ï¼ˆcode + name_enï¼‰


# ----------------------------
# å¼‚å¸¸ç±»å‹
# ----------------------------
class ConfigError(RuntimeError):
    """ç”¨äºå¯åŠ¨é˜¶æ®µçš„é…ç½®é”™è¯¯ï¼ˆæ›´å‹å¥½çš„æŠ¥é”™ä¸è§£å†³å»ºè®®ï¼‰"""
    pass


# ----------------------------
# æ•°æ®æ¨¡å‹ï¼ˆæŒ‰ strings_i18n.yaml schemaï¼‰
# ----------------------------
@dataclass(frozen=True)
class Locale:
    code: str
    name_en: str


@dataclass(frozen=True)
class StringsI18nConfig:
    # è·¯å¾„
    project_root: Path
    languages_path: Path          # ç»å¯¹è·¯å¾„
    lang_root: Path               # ç»å¯¹è·¯å¾„ï¼š*.lproj æ‰€åœ¨ç›®å½•
    base_folder: str              # e.g. Base.lproj

    # è¯­è¨€
    base_locale: Locale
    source_locale: Locale
    core_locales: List[Locale]
    target_locales: List[Locale]

    # è¡Œä¸ºå¼€å…³
    options: Dict[str, Any]
    prompts: Dict[str, Any]


# ----------------------------
# å†…ç½®æ–‡ä»¶è¯»å–ï¼ˆæ¨¡æ¿ / é»˜è®¤ languages.jsonï¼‰
# ----------------------------
def _pkg_file(name: str) -> Path:
    # é»˜è®¤æŠŠæ¨¡æ¿ä¸é»˜è®¤ languages.json æ”¾åœ¨ data.py åŒç›®å½•
    return Path(__file__).with_name(name)


def ensure_languages_json(project_root: Path, languages_rel: str = DEFAULT_LANGUAGES_NAME) -> Path:
    """å¦‚æœæœ¬åœ°æ²¡æœ‰ languages.jsonï¼Œåˆ™ç”¨å†…ç½®é»˜è®¤ languages.json ç”Ÿæˆä¸€ä»½ã€‚"""
    project_root = project_root.resolve()
    dst = (project_root / languages_rel).resolve()

    if dst.exists():
        return dst

    src = _pkg_file(DEFAULT_LANGUAGES_NAME)
    if not src.exists():
        raise FileNotFoundError(f"å†…ç½®é»˜è®¤ {DEFAULT_LANGUAGES_NAME} ä¸å­˜åœ¨ï¼š{src}")

    dst.parent.mkdir(parents=True, exist_ok=True)
    dst.write_text(src.read_text(encoding="utf-8"), encoding="utf-8")
    return dst


def _load_languages(languages_path: Path) -> List[Dict[str, str]]:
    arr = json.loads(languages_path.read_text(encoding="utf-8"))
    if not isinstance(arr, list):
        raise ValueError(f"{languages_path.name} é¡¶å±‚å¿…é¡»æ˜¯æ•°ç»„")
    out: List[Dict[str, str]] = []
    for item in arr:
        if not isinstance(item, dict):
            continue
        code = str(item.get("code", "")).strip()
        name_en = str(item.get("name_en", "")).strip()
        if not code or not name_en:
            continue
        out.append({"code": code, "name_en": name_en})
    return out


def _all_locale_codes(cfg: StringsI18nConfig) -> List[str]:
    codes: List[str] = []
    for loc in [cfg.base_locale, cfg.source_locale] + cfg.core_locales + cfg.target_locales:
        if loc and loc.code not in codes:
            codes.append(loc.code)
    return codes


def _dedup_locales_preserve_order(locales: List[Locale]) -> List[Locale]:
    seen: set[str] = set()
    out: List[Locale] = []
    for l in locales:
        if l.code in seen:
            continue
        seen.add(l.code)
        out.append(l)
    return out


_PRINTF_RE = re.compile(r'%(?:\d+\$)?(?:@|d|i|u|f|s|ld|lld|lu|llu|lf)', re.IGNORECASE)

def _extract_printf_placeholders(value: str) -> List[str]:
    # å¿½ç•¥è½¬ä¹‰çš„ %%ï¼ˆå®ƒä¸æ˜¯å ä½ç¬¦ï¼‰
    if not value:
        return []
    # ä¸´æ—¶æ›¿æ¢ %% é˜²æ­¢è¢«æ­£åˆ™è¯¯ä¼¤
    tmp = value.replace("%%", "")
    return _PRINTF_RE.findall(tmp)


def _doctor_print_and_write(
    cfg: StringsI18nConfig,
    errors: List[str],
    warns: List[str],
    extra_sections: Optional[Dict[str, Any]] = None,
) -> int:
    # æ§åˆ¶å°æ‘˜è¦
    print("\n=== doctor summary ===")
    print(f"- project_root: {cfg.project_root}")
    print(f"- lang_root:    {cfg.lang_root}")
    print(f"- base_folder:  {cfg.base_folder}")
    print(f"- base_locale:  {cfg.base_locale.code}")
    print(f"- source_locale:{cfg.source_locale.code}")
    print(f"- core_locales: {[l.code for l in cfg.core_locales]}")
    print(f"- target_locales: {len(cfg.target_locales)}")

    if errors:
        print("\n[ERROR]")
        for e in errors:
            print(f"- {e}")
    if warns:
        print("\n[WARN]")
        for w in warns:
            print(f"- {w}")

    # å†™æŠ¥å‘Šæ–‡ä»¶ï¼ˆå«è¯¦ç»† sectionï¼‰
    try:
        lines: List[str] = []
        lines.append("box_strings_i18n doctor report")
        lines.append("")
        lines.append("=== summary ===")
        lines.append(f"project_root: {cfg.project_root}")
        lines.append(f"lang_root:    {cfg.lang_root}")
        lines.append(f"base_folder:  {cfg.base_folder}")
        lines.append(f"base_locale:  {cfg.base_locale.code}")
        lines.append(f"source_locale:{cfg.source_locale.code}")
        lines.append(f"core_locales: {[l.code for l in cfg.core_locales]}")
        lines.append(f"target_locales: {len(cfg.target_locales)}")

        if errors:
            lines.append("")
            lines.append("[ERROR]")
            for e in errors:
                lines.append(f"- {e}")
        if warns:
            lines.append("")
            lines.append("[WARN]")
            for w in warns:
                lines.append(f"- {w}")

        if extra_sections:
            lines.append("")
            lines.append("=== details ===")
            for k, v in (extra_sections or {}).items():
                lines.append("")
                lines.append(f"## {k}")
                if isinstance(v, str):
                    lines.append(v.rstrip())
                else:
                    lines.append(pprint.pformat(v, width=120))

        content = "\n".join(lines).rstrip() + "\n"
        report_path = _write_report_file(cfg, content, name="doctor")
        if report_path is not None:
            print(f"\nReport: {report_path}")
    except Exception as e:
        print(f"\nReport å†™å…¥å¤±è´¥ï¼š{e}")

    return 1 if errors else 0


def build_target_locales_from_languages_json(
    languages_path: Path,
    *,
    source_code: str,
    core_codes: List[str],
) -> Tuple[List[Dict[str, str]], int]:
    """
    ä» languages.json ç”Ÿæˆ target_localesï¼ˆcode + name_enï¼‰ï¼Œå¹¶ï¼š
    - æŒ‰ code å»é‡ï¼ˆä¿åºï¼‰
    - å‰”é™¤ source_code
    - å‰”é™¤ core_codes
    è¿”å›ï¼š(targets, removed_count)
    """
    items = _load_languages(languages_path)
    seen = set()
    out: List[Dict[str, str]] = []
    removed = 0

    core_set = set(core_codes)

    for it in items:
        code = it["code"]
        if code == source_code or code in core_set:
            removed += 1
            continue
        if code in seen:
            continue
        seen.add(code)
        out.append(it)

    return out, removed


# ----------------------------
# YAML æ¨¡æ¿â€œä¿æ³¨é‡Šâ€å±€éƒ¨æ›¿æ¢ï¼štarget_locales block
# ----------------------------
def _yaml_block_for_target_locales(locales: List[Dict[str, str]]) -> str:
    lines = ["target_locales:"]
    for it in locales:
        lines.append(f"  - code: {it['code']}")
        lines.append(f"    name_en: {it['name_en']}")
    return "\n".join(lines) + "\n"


def replace_target_locales_block(template_text: str, new_locales: List[Dict[str, str]]) -> str:
    """
    ä»…æ›¿æ¢æ¨¡æ¿ä¸­ `target_locales:` æ®µè½çš„å†…å®¹ï¼Œå…¶ä»–æ³¨é‡Š/æ’ç‰ˆä¿ç•™ã€‚
    åŒ¹é…è§„åˆ™ï¼šä» `target_locales:` å¼€å§‹ï¼Œæ›¿æ¢åˆ°ä¸‹ä¸€ä¸ªé¡¶å±‚ key ä¹‹å‰ã€‚
    """
    new_block = _yaml_block_for_target_locales(new_locales)

    start_match = re.search(r"(?m)^target_locales:\s*$", template_text)
    if not start_match:
        raise ValueError("æ¨¡æ¿ä¸­æœªæ‰¾åˆ° target_locales: æ®µè½")

    start = start_match.start()
    after = template_text[start_match.end():]

    # ä¸‹ä¸€æ®µé¡¶å±‚ keyï¼ˆå½¢å¦‚ prompts:, options:, languages: ç­‰ï¼‰
    next_key = re.search(r"(?m)^(?!target_locales:)[A-Za-z_][A-Za-z0-9_]*:\s*$", after)

    if next_key:
        end = start_match.end() + next_key.start()
    else:
        end = len(template_text)

    return template_text[:start] + new_block + template_text[end:]


# ----------------------------
# initï¼šç”Ÿæˆ/æ ¡éªŒé…ç½®ï¼Œç¡®ä¿ languages.json + lang_root å­˜åœ¨
# ----------------------------
def init_config(project_root: Path, cfg_path: Path) -> None:
    project_root = project_root.resolve()
    cfg_path = cfg_path.resolve()

    # 1) cfg ä¸å­˜åœ¨ï¼šç”¨å†…ç½®æ¨¡æ¿ç”Ÿæˆï¼ˆä¿ç•™æ³¨é‡Šï¼‰+ åŠ¨æ€æ›¿æ¢ target_locales
    if not cfg_path.exists():
        tpl = _pkg_file(DEFAULT_TEMPLATE_NAME)
        if not tpl.exists():
            raise FileNotFoundError(f"å†…ç½®é»˜è®¤é…ç½®æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl}")

        tpl_text = tpl.read_text(encoding="utf-8")
        raw_tpl = yaml.safe_load(tpl_text) or {}
        validate_config(raw_tpl)  # æ¨¡æ¿è‡ªèº«ä¹Ÿè¦åˆæ³•

        # 2) å…ˆç¡®ä¿ languages.json å­˜åœ¨ï¼ˆæŒ‰æ¨¡æ¿é‡Œçš„ languages å­—æ®µï¼‰
        languages_rel = str(raw_tpl.get("languages") or DEFAULT_LANGUAGES_NAME)
        languages_path = ensure_languages_json(project_root, languages_rel=languages_rel)

        # 3) ç”Ÿæˆ targetsï¼šlanguages - core - source
        src = _first_locale(raw_tpl["source_locale"])
        core = [_locale_obj(x) for x in (raw_tpl.get("core_locales") or [])]
        targets, _removed = build_target_locales_from_languages_json(
            languages_path,
            source_code=src.code,
            core_codes=[c.code for c in core],
        )

        out_text = replace_target_locales_block(tpl_text, targets)
        cfg_path.parent.mkdir(parents=True, exist_ok=True)
        cfg_path.write_text(out_text, encoding="utf-8")

    # 4) æ ¡éªŒé…ç½®ï¼ˆinit é˜¶æ®µä¸å¼ºåˆ¶æ£€æŸ¥ç›®å½•å­˜åœ¨ï¼‰
    assert_config_ok(cfg_path, project_root=project_root, check_paths_exist=False)

    # 5) åˆ›å»º lang_root ç›®å½•ï¼ˆæŒ‰ project_root è§£æï¼‰
    raw = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
    lang_root = (project_root / str(raw["lang_root"])).resolve()
    lang_root.mkdir(parents=True, exist_ok=True)

    # 6) ç¡®ä¿ languages æ–‡ä»¶å­˜åœ¨ï¼ˆæŒ‰é…ç½®ï¼‰
    languages_rel = str(raw.get("languages") or DEFAULT_LANGUAGES_NAME)
    ensure_languages_json(project_root, languages_rel=languages_rel)


# ----------------------------
# å¯åŠ¨ä¼˜å…ˆæ ¡éªŒå…¥å£
# ----------------------------
def assert_config_ok(
    cfg_path: Path,
    *,
    project_root: Optional[Path] = None,
    check_paths_exist: bool = True,
) -> Dict[str, Any]:
    cfg_path = cfg_path.resolve()
    project_root = (project_root or cfg_path.parent).resolve()

    if not cfg_path.exists():
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{cfg_path}\n"
            f"è§£å†³æ–¹æ³•ï¼šè¿è¡Œ `box_strings_i18n init` ç”Ÿæˆé»˜è®¤é…ç½®ã€‚"
        )

    try:
        raw = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
    except Exception as e:
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶æ— æ³•è§£æä¸º YAMLï¼š{cfg_path}\n"
            f"åŸå› ï¼š{e}\n"
            f"è§£å†³æ–¹æ³•ï¼šä¿®å¤ YAML æ ¼å¼æˆ–è¿è¡Œ `box_strings_i18n init` é‡æ–°ç”Ÿæˆã€‚"
        )

    try:
        validate_config(raw)
    except Exception as e:
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶æ ¡éªŒå¤±è´¥ï¼š{cfg_path}\n"
            f"åŸå› ï¼š{e}\n"
            f"è§£å†³æ–¹æ³•ï¼šä¿®å¤é…ç½®å­—æ®µ/ç±»å‹ï¼Œæˆ–è¿è¡Œ `box_strings_i18n init` é‡æ–°ç”Ÿæˆã€‚"
        )

    if check_paths_exist:
        # languages
        languages_path = (project_root / str(raw["languages"])).resolve()
        if not languages_path.exists():
            raise ConfigError(
                f"languages æ–‡ä»¶ä¸å­˜åœ¨ï¼š{languages_path}\n"
                f"è§£å†³æ–¹æ³•ï¼šè¿è¡Œ `box_strings_i18n init` è‡ªåŠ¨ç”Ÿæˆï¼Œæˆ–ä¿®å¤é…ç½®ä¸­çš„ languages è·¯å¾„ã€‚"
            )

        # lang_root + base_folder
        lang_root = (project_root / str(raw["lang_root"])).resolve()
        if not lang_root.exists():
            raise ConfigError(
                f"lang_root ç›®å½•ä¸å­˜åœ¨ï¼š{lang_root}\n"
                f"è§£å†³æ–¹æ³•ï¼šåˆ›å»ºç›®å½•æˆ–è¿è¡Œ `box_strings_i18n init` è®©å·¥å…·åˆå§‹åŒ–ã€‚"
            )

        base_folder = str(raw["base_folder"])
        base_dir = (lang_root / base_folder).resolve()
        if not base_dir.exists():
            raise ConfigError(
                f"Base è¯­è¨€ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}\n"
                f"è§£å†³æ–¹æ³•ï¼šç¡®è®¤ Xcode å·¥ç¨‹å†… Base.lproj è·¯å¾„ï¼Œæˆ–ä¿®å¤é…ç½®ä¸­çš„ lang_root/base_folderã€‚"
            )

    return raw


# ----------------------------
# load_configï¼šæŠŠ raw dict è½¬æˆ StringsI18nConfigï¼ˆè·¯å¾„è§£æä¸ºç»å¯¹è·¯å¾„ï¼‰
# ----------------------------
def load_config(cfg_path: Path, *, project_root: Optional[Path] = None) -> StringsI18nConfig:
    cfg_path = cfg_path.resolve()
    project_root = (project_root or cfg_path.parent).resolve()

    raw = assert_config_ok(cfg_path, project_root=project_root, check_paths_exist=True)

    languages_path = (project_root / str(raw["languages"])).resolve()
    lang_root = (project_root / str(raw["lang_root"])).resolve()

    base_locale = _first_locale(raw["base_locale"])
    source_locale = _first_locale(raw["source_locale"])
    core_locales = [_locale_obj(x) for x in (raw.get("core_locales") or [])]
    target_locales = [_locale_obj(x) for x in (raw.get("target_locales") or [])]

    return StringsI18nConfig(
        project_root=project_root,
        languages_path=languages_path,
        lang_root=lang_root,
        base_folder=str(raw["base_folder"]),
        base_locale=base_locale,
        source_locale=source_locale,
        core_locales=core_locales,
        target_locales=target_locales,
        options=dict(raw.get("options") or {}),
        prompts=dict(raw.get("prompts") or {}),
    )


# ----------------------------
# validate_configï¼šå­—æ®µ + ç±»å‹ + å…³é”®è¯­ä¹‰æ ¡éªŒ
# ----------------------------
def validate_config(raw: Dict[str, Any]) -> None:
    required_top = [
        "options", "languages", "lang_root", "base_folder",
        "base_locale", "source_locale", "core_locales",
        "target_locales", "prompts",
    ]
    for k in required_top:
        if k not in raw:
            raise ValueError(f"é…ç½®ç¼ºå°‘å­—æ®µï¼š{k}")

    # options
    options = raw["options"]
    if not isinstance(options, dict):
        raise ValueError("options å¿…é¡»æ˜¯ object")

    for k in ["cleanup_extra_keys", "incremental_translate", "normalize_filenames", "sort_keys"]:
        if k not in options:
            raise ValueError(f"options ç¼ºå°‘å­—æ®µï¼š{k}")

    # paths
    for k in ["languages", "lang_root", "base_folder"]:
        if not isinstance(raw[k], str) or not str(raw[k]).strip():
            raise ValueError(f"{k} å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")

    # locales (è¿™äº›åœ¨æ¨¡æ¿é‡Œæ˜¯ list[object]ï¼Œæ¯ä¸ªåªæ”¾ä¸€ä¸ª)
    _ = _first_locale(raw["base_locale"])
    src = _first_locale(raw["source_locale"])

    core = raw["core_locales"]
    if not isinstance(core, list) or len(core) == 0:
        raise ValueError("core_locales å¿…é¡»æ˜¯éç©ºæ•°ç»„")
    core_locales = [_locale_obj(x) for x in core]

    targets = raw["target_locales"]
    if not isinstance(targets, list):
        raise ValueError("target_locales å¿…é¡»æ˜¯æ•°ç»„ï¼ˆå…è®¸ä¸ºç©ºï¼Œä½†å»ºè®®ç”± init ç”Ÿæˆï¼‰")
    target_locales = [_locale_obj(x) for x in targets]

    # è¯­ä¹‰ï¼šå»é‡ä¸å†²çª
    def codes(locales: List[Locale]) -> List[str]:
        return [x.code for x in locales]

    core_codes = codes(core_locales)
    if len(set(core_codes)) != len(core_codes):
        raise ValueError("core_locales.code å­˜åœ¨é‡å¤ï¼Œè¯·å»é‡")

    tgt_codes = codes(target_locales)
    if len(set(tgt_codes)) != len(tgt_codes):
        raise ValueError("target_locales.code å­˜åœ¨é‡å¤ï¼Œè¯·å»é‡")

    if src.code in set(tgt_codes):
        raise ValueError("target_locales é‡ŒåŒ…å« source_locale.codeï¼Œè¯·ç§»é™¤ï¼ˆsource ä¸èƒ½ä½œä¸º targetï¼‰")

    # prompts
    prompts = raw["prompts"]
    if not isinstance(prompts, dict):
        raise ValueError("prompts å¿…é¡»æ˜¯ object")
    if "default_en" not in prompts or not isinstance(prompts["default_en"], str):
        raise ValueError("prompts.default_en å¿…é¡»å­˜åœ¨ä¸”ä¸ºå­—ç¬¦ä¸²")


def _locale_obj(obj: Any) -> Locale:
    if not isinstance(obj, dict):
        raise ValueError("locale item å¿…é¡»æ˜¯ object")
    code = str(obj.get("code", "")).strip()
    name_en = str(obj.get("name_en", "")).strip()
    if not code or not name_en:
        raise ValueError("locale.code/name_en ä¸èƒ½ä¸ºç©º")
    return Locale(code=code, name_en=name_en)


def _first_locale(obj: Any) -> Locale:
    if not isinstance(obj, list) or len(obj) == 0:
        raise ValueError("locale å¿…é¡»æ˜¯éç©ºæ•°ç»„ï¼ˆlistï¼‰ï¼Œä¸”ç¬¬ä¸€é¡¹ä¸º object")
    return _locale_obj(obj[0])


# ----------------------------
# commandsï¼šdoctor/sortï¼ˆéª¨æ¶ï¼‰
# ----------------------------
def run_doctor(cfg: StringsI18nConfig) -> int:
    """
    æœ€ä½³å®è·µçš„ doctorï¼š
    - é…ç½® & ç›®å½•ç»“æ„æ ¡éªŒ
    - Base.lproj/å…¶å®ƒè¯­è¨€ *.strings å¯è§£ææ€§æ£€æŸ¥
    - key ä¸€è‡´æ€§ï¼ˆç¼ºå¤±/å†—ä½™ï¼‰ç»Ÿè®¡
    - é‡å¤ key æ£€æµ‹ï¼ˆBase è§†ä¸ºé”™è¯¯ï¼›å…¶å®ƒè¯­è¨€è§†ä¸ºè­¦å‘Šï¼‰
    - Swift camelCase å†²çªæ£€æµ‹ï¼ˆBase è§†ä¸ºé”™è¯¯ï¼šä¼šå¯¼è‡´ L10n.swift å±æ€§åå†²çªï¼‰
    - printf å ä½ç¬¦ä¸€è‡´æ€§ï¼ˆ%@/%d/%1$@ ...ï¼‰æ£€æŸ¥ï¼ˆè­¦å‘Šï¼‰
    """
    errors: List[str] = []
    warns: List[str] = []

    # ---- è·¯å¾„/ç»“æ„ ----
    if not cfg.lang_root.exists():
        errors.append(f"lang_root ä¸å­˜åœ¨ï¼š{cfg.lang_root}")
        return _doctor_print_and_write(cfg, errors, warns)

    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        errors.append(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")
        return _doctor_print_and_write(cfg, errors, warns)

    if not cfg.languages_path.exists():
        errors.append(f"languages.json ä¸å­˜åœ¨ï¼š{cfg.languages_path}ï¼ˆå¯å…ˆæ‰§è¡Œ initï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆæ¨¡æ¿/æ‹·è´é»˜è®¤ languages.jsonï¼‰")
        return _doctor_print_and_write(cfg, errors, warns)

    # ---- languages.json å†…å®¹ ----
    try:
        languages_list = _load_languages(cfg.languages_path)
        languages = {d['code'] for d in languages_list if 'code' in d}
    except Exception as e:
        errors.append(f"languages.json è¯»å–å¤±è´¥ï¼š{cfg.languages_path}ï¼ˆ{e}ï¼‰")
        return _doctor_print_and_write(cfg, errors, warns)

    cfg_codes = _all_locale_codes(cfg)
    missing_in_languages = [c for c in cfg_codes if c not in languages]
    if missing_in_languages:
        warns.append(
            "languages.json ç¼ºå°‘ä»¥ä¸‹ codeï¼ˆå»ºè®®è¡¥å…¨ï¼Œä»¥ä¾¿ init/æ ¡éªŒä¸€è‡´ï¼‰ï¼š"
            + ", ".join(missing_in_languages)
        )

    # ---- Base.lproj æ–‡ä»¶é›† ----
    base_files = sorted([p for p in base_dir.glob("*.strings") if p.is_file()])
    if not base_files:
        errors.append(f"Base ç›®å½•ä¸‹æœªå‘ç°ä»»ä½• *.stringsï¼š{base_dir}")
        return _doctor_print_and_write(cfg, errors, warns)

    # è§£æ Base å¹¶å»ºç«‹â€œé‡‘æ ‡å‡† key é›†åˆâ€
    base_map: Dict[str, List[StringsEntry]] = {}
    base_keys_by_file: Dict[str, set] = {}

    # âœ… æ–°å¢ï¼šSwift camelCase å†²çªï¼ˆæŒ‰æ–‡ä»¶ï¼‰
    base_camel_conflicts_by_file: Dict[str, Dict[str, List[str]]] = {}

    for fp in base_files:
        try:
            preamble, entries = parse_strings_file(fp)
        except Exception as e:
            errors.append(f"Base è§£æå¤±è´¥ï¼š{fp.name}ï¼ˆ{e}ï¼‰")
            continue

        dups = _collect_duplicates(entries)
        if dups:
            errors.append(f"Base å­˜åœ¨é‡å¤ keyï¼š{fp.name} -> {dups}")

        # âœ… camelCase å†²çªï¼šBase è§†ä¸º ERRORï¼ˆgen L10n.swift ä¼šæ’å±æ€§åï¼‰
        camel_conflicts = scan_camelcase_conflicts(entries)
        if camel_conflicts:
            base_camel_conflicts_by_file[fp.name] = camel_conflicts
            errors.append(
                "Base å­˜åœ¨ Swift camelCase å±æ€§åå†²çªï¼ˆä¼šå¯¼è‡´ L10n.swift ç¼–è¯‘/ç”Ÿæˆå¤±è´¥ï¼‰ï¼š"
                f"{fp.name} -> { {k: v for k, v in list(camel_conflicts.items())[:10]} }"
                + (" â€¦" if len(camel_conflicts) > 10 else "")
            )

        base_map[fp.name] = entries
        base_keys_by_file[fp.name] = {e.key for e in entries}

    # ---- å…¶å®ƒè¯­è¨€æ£€æŸ¥ ----
    other_locales = [cfg.source_locale] + cfg.core_locales + cfg.target_locales
    other_locales = _dedup_locales_preserve_order(other_locales)

    missing_dirs: List[str] = []
    missing_files: List[str] = []
    parse_fail: List[str] = []

    missing_keys: Dict[str, Dict[str, List[str]]] = {}
    redundant_keys: Dict[str, Dict[str, List[str]]] = {}

    placeholder_mismatch: Dict[str, Dict[str, List[Tuple[str, List[str], List[str]]]]] = {}

    for loc in other_locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            missing_dirs.append(loc.code)
            continue

        for bf in base_files:
            target_fp = (loc_dir / bf.name)
            if not target_fp.exists():
                missing_files.append(f"{loc.code}/{bf.name}")
                continue

            try:
                _, loc_entries = parse_strings_file(target_fp)
            except Exception as e:
                parse_fail.append(f"{loc.code}/{bf.name}ï¼ˆ{e}ï¼‰")
                continue

            dups = _collect_duplicates(loc_entries)
            if dups:
                warns.append(f"é‡å¤ keyï¼ˆ{loc.code}/{bf.name}ï¼‰ï¼š{dups}")

            base_keys = base_keys_by_file.get(bf.name, set())
            loc_keys = {e.key for e in loc_entries}

            mk = sorted(list(base_keys - loc_keys))
            rk = sorted(list(loc_keys - base_keys))

            if mk:
                missing_keys.setdefault(loc.code, {}).setdefault(bf.name, []).extend(mk)
            if rk:
                redundant_keys.setdefault(loc.code, {}).setdefault(bf.name, []).extend(rk)

            base_entries_by_key = {e.key: e for e in base_map.get(bf.name, [])}
            loc_entries_by_key = {e.key: e for e in loc_entries}
            for k in (base_keys & loc_keys):
                b = base_entries_by_key.get(k)
                t = loc_entries_by_key.get(k)
                if not b or not t:
                    continue
                bph = _extract_printf_placeholders(b.value)
                tph = _extract_printf_placeholders(t.value)
                if bph != tph:
                    placeholder_mismatch.setdefault(loc.code, {}).setdefault(bf.name, []).append((k, bph, tph))

    if missing_dirs:
        warns.append("ç¼ºå°‘è¯­è¨€ç›®å½•ï¼ˆå¯é€šè¿‡ sort è‡ªåŠ¨è¡¥é½ç©ºæ–‡ä»¶å¤¹/æ–‡ä»¶ï¼‰ï¼š"
                     + ", ".join(sorted(set(missing_dirs))))
    if missing_files:
        warns.append("ç¼ºå°‘ *.strings æ–‡ä»¶ï¼ˆå¯é€šè¿‡ sort è‡ªåŠ¨åˆ›å»ºç©ºæ–‡ä»¶ï¼‰ï¼š"
                     + ", ".join(missing_files[:30]) + (" â€¦" if len(missing_files) > 30 else ""))

    if parse_fail:
        errors.append("ä»¥ä¸‹æ–‡ä»¶è§£æå¤±è´¥ï¼ˆè¯·å…ˆä¿®å¤è¯­æ³•/å¼•å·/åˆ†å·ç­‰ï¼‰ï¼š"
                      + "; ".join(parse_fail[:20]) + (" â€¦" if len(parse_fail) > 20 else ""))

    miss_count = sum(len(keys) for m in missing_keys.values() for keys in m.values())
    red_count = sum(len(keys) for m in redundant_keys.values() for keys in m.values())
    ph_count = sum(len(v) for m in placeholder_mismatch.values() for v in m.values())

    if miss_count:
        warns.append(f"å‘ç°ç¼ºå¤± keyï¼ˆç›¸å¯¹ Baseï¼‰ï¼šå…± {miss_count} ä¸ªï¼ˆå»ºè®®èµ° translate å¢é‡æˆ–è¡¥é½ï¼‰")
    if red_count:
        warns.append(f"å‘ç°å†—ä½™ keyï¼ˆBase ä¸å­˜åœ¨ï¼‰ï¼šå…± {red_count} ä¸ªï¼ˆå»ºè®®åœ¨ sort ä¸­é€‰æ‹©åˆ é™¤ï¼‰")
    if ph_count:
        warns.append(f"å‘ç°å ä½ç¬¦ä¸ä¸€è‡´ï¼šå…± {ph_count} é¡¹ï¼ˆå»ºè®®äººå·¥ç¡®è®¤ï¼Œé¿å…è¿è¡Œæ—¶å´©æºƒ/æ ¼å¼é”™ä¹±ï¼‰")

    if ph_count:
        policy = _resolve_placeholder_mismatch_policy(cfg, placeholder_mismatch, max_items=3)
        if policy == "delete":
            n = _apply_placeholder_mismatch_delete(cfg, placeholder_mismatch)
            warns.append(f"å·²åˆ é™¤å ä½ç¬¦ä¸ä¸€è‡´æ¡ç›®ï¼š{n} æ¡ï¼ˆå»ºè®®å†è¿è¡Œ translate å¢é‡è¡¥é½ï¼‰")

    if red_count:
        preview_report: Dict[str, List[str]] = {}
        for lang, by_file in sorted(redundant_keys.items(), key=lambda kv: kv[0]):
            for fn, keys in sorted(by_file.items(), key=lambda kv: kv[0]):
                for k in sorted(set(keys)):
                    preview_report.setdefault(lang, []).append(f"{fn}:{k}")
        content = _format_key_report(preview_report, title="âš ï¸ å†—ä½™ keyï¼ˆç¤ºä¾‹é¢„è§ˆï¼‰ï¼š", max_keys_per_file=4)
        print(content)
        p = _write_report_file(cfg, content, name="redundant_keys_preview")
        if p is not None:
            print(f"ğŸ“„ å·²è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ï¼š{p}")

        opt = (cfg.options or {}).get("redundant_key_policy")
        if opt in {"keep", "delete"}:
            pass
        elif sys.stdin.isatty():
            ans = input("æ˜¯å¦ç°åœ¨å°±åˆ é™¤è¿™äº›å†—ä½™ keyï¼Ÿ(y=åˆ é™¤ / n=ä¿ç•™ç»§ç»­) [n]: ").strip().lower()
            if ans == "y":
                deleted = 0
                for lang, by_file in redundant_keys.items():
                    loc_dir = (cfg.lang_root / f"{lang}.lproj").resolve()
                    if not loc_dir.exists():
                        continue
                    for fn, keys in by_file.items():
                        fp = (loc_dir / fn).resolve()
                        if not fp.exists():
                            continue
                        try:
                            preamble, entries = parse_strings_file(fp)
                        except Exception:
                            continue
                        bad = set(keys)
                        new_entries = [e for e in entries if e.key not in bad]
                        if len(new_entries) != len(entries):
                            deleted += (len(entries) - len(new_entries))
                            write_strings_file(fp, preamble, new_entries, group_by_prefix=False)
                warns.append(f"å·²åˆ é™¤å†—ä½™ keyï¼š{deleted} æ¡")

    strict = bool(cfg.options.get("doctor_strict", False))
    if strict and warns:
        errors.extend([f"[STRICT] {w}" for w in warns])
        warns = []

    return _doctor_print_and_write(
        cfg,
        errors,
        warns,
        extra_sections={
            "ç¼ºå¤± keyï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": missing_keys,
            "å†—ä½™ keyï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": redundant_keys,
            "å ä½ç¬¦ä¸ä¸€è‡´ï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": placeholder_mismatch,
            "Base Swift camelCase å†²çªï¼ˆæŒ‰æ–‡ä»¶ï¼‰": base_camel_conflicts_by_file,
        },
    )


# ----------------------------
# sort å‰çš„å®Œæ•´æ€§æ£€æŸ¥ï¼šç¡®ä¿å„è¯­è¨€ *.lproj ç›®å½•ä¸ Base.lproj çš„ *.strings æ–‡ä»¶é›†ä¸€è‡´
# - è‹¥ç¼ºå¤±ï¼šåˆ›å»ºç›®å½•ä¸ç©ºæ–‡ä»¶
# - è‹¥å¤šä½™ï¼šæš‚ä¸åˆ é™¤ï¼ˆé¿å…è¯¯åˆ é¡¹ç›®è‡ªå®šä¹‰æ–‡ä»¶ï¼‰
# ----------------------------
def ensure_strings_files_integrity(cfg: StringsI18nConfig) -> Tuple[int, int]:
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        raise ConfigError(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")

    base_strings = sorted([p for p in base_dir.glob('*.strings') if p.is_file()])
    if not base_strings:
        raise ConfigError(
            f"Base ç›®å½•ä¸‹æœªå‘ç°ä»»ä½• .strings æ–‡ä»¶ï¼š{base_dir}\n"
            f"è§£å†³æ–¹æ³•ï¼šç¡®è®¤ Xcode æ˜¯å¦å·²ç”Ÿæˆ Localizable.strings ç­‰æ–‡ä»¶ï¼Œæˆ–æ£€æŸ¥ lang_root/base_folder é…ç½®ã€‚"
        )

    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    created_dirs = 0
    created_files = 0

    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            loc_dir.mkdir(parents=True, exist_ok=True)
            created_dirs += 1

        existing = {p.name for p in loc_dir.glob('*.strings') if p.is_file()}
        for base_file in base_strings:
            if base_file.name not in existing:
                target = loc_dir / base_file.name
                target.write_text('', encoding='utf-8')
                created_files += 1

    return created_dirs, created_files


# ----------------------------
# .strings è§£æ/å†™å› + æ’åº
# ----------------------------
_STRINGS_ENTRY_RE = re.compile(r'^\s*"((?:\\.|[^"\\])*)"\s*=\s*"((?:\\.|[^"\\])*)"\s*;\s*$')

@dataclass
class StringsEntry:
    key: str
    value: str
    comments: List[str]  # åŸæ ·ä¿å­˜ï¼ˆè¡Œçº§ï¼‰ï¼Œå†™å›æ—¶æ”¾åœ¨ entry ä¸Šæ–¹


def _is_comment_line(line: str) -> bool:
    s = line.lstrip()
    return s.startswith("//") or s.startswith("/*") or s.startswith("*") or s.startswith("*/")


def _group_prefix(key: str) -> str:
    # è§„åˆ™ï¼šä¼˜å…ˆæŒ‰ '.' çš„ç¬¬ä¸€ä¸ªæ®µï¼›å¦åˆ™æŒ‰ '_' çš„ç¬¬ä¸€ä¸ªæ®µï¼›å¦åˆ™å…¨ key
    if "." in key:
        return key.split(".", 1)[0]
    if "_" in key:
        return key.split("_", 1)[0]
    return key


def parse_strings_file(path: Path) -> Tuple[List[str], List[StringsEntry]]:
    """è§£æ iOS .strings æ–‡ä»¶ï¼Œä¿ç•™æ³¨é‡Šï¼ˆæ³¨é‡Šå½’å±åˆ°å…¶ä¸‹æ–¹çš„ keyï¼‰ã€‚"""
    if not path.exists():
        return [], []

    lines = path.read_text(encoding="utf-8").splitlines()
    preamble: List[str] = []
    entries: List[StringsEntry] = []

    pending_comments: List[str] = []
    seen_first_entry = False

    for line in lines:
        m = _STRINGS_ENTRY_RE.match(line)
        if m:
            key, value = m.group(1), m.group(2)

            while pending_comments and pending_comments[-1].strip() == "":
                pending_comments.pop()

            entries.append(StringsEntry(key=key, value=value, comments=pending_comments))
            pending_comments = []
            seen_first_entry = True
            continue

        if not seen_first_entry:
            preamble.append(line)
            continue

        if line.strip() == "" or _is_comment_line(line):
            pending_comments.append(line)
        else:
            pending_comments.append(line)

    return preamble, entries


def write_strings_file(path: Path, preamble: List[str], entries: List[StringsEntry], *, group_by_prefix: bool = True) -> None:
    out_lines: List[str] = []

    if preamble:
        while preamble and preamble[-1].strip() == "":
            preamble.pop()
        out_lines.extend(preamble)
        out_lines.append("")

    prev_group: Optional[str] = None
    for e in entries:
        grp = _group_prefix(e.key)
        if prev_group is not None and grp != prev_group:
            out_lines.append("")
        prev_group = grp

        if e.comments:
            comments = list(e.comments)
            while comments and comments[0].strip() == "":
                comments.pop(0)
            while comments and comments[-1].strip() == "":
                comments.pop()
            out_lines.extend(comments)

        out_lines.append(f"\"{e.key}\" = \"{e.value}\";")

    path.write_text("\n".join(out_lines) + "\n", encoding="utf-8")


def sort_strings_entries(preamble: List[str], entries: List[StringsEntry]) -> Tuple[List[str], List[StringsEntry]]:
    entries_sorted = sorted(entries, key=lambda e: (_group_prefix(e.key), e.key))
    return preamble, entries_sorted


def _collect_duplicates(entries: List[StringsEntry]) -> List[str]:
    seen = set()
    dups = set()
    for e in entries:
        if e.key in seen:
            dups.add(e.key)
        else:
            seen.add(e.key)
    return sorted(dups)


def _apply_duplicate_policy(entries: List[StringsEntry], policy: str) -> List[StringsEntry]:
    """å¤„ç†é‡å¤ keyã€‚policy:
    - keep_first: åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ key
    - delete_all: é‡å¤ keyï¼ˆå‡ºç°>=2ï¼‰å…¨éƒ¨åˆ é™¤
    """
    if policy not in {"keep_first", "delete_all"}:
        return entries

    dups = set(_collect_duplicates(entries))
    if not dups:
        return entries

    if policy == "keep_first":
        kept = []
        seen = set()
        for e in entries:
            if e.key in dups:
                if e.key in seen:
                    continue
                seen.add(e.key)
            kept.append(e)
        return kept

    return [e for e in entries if e.key not in dups]


def _base_keys_by_file(cfg: StringsI18nConfig) -> Dict[str, set]:
    """è¯»å– Base.lproj ä¸‹æ¯ä¸ª *.strings çš„ key é›†åˆã€‚key ç”¨äºåˆ¤å®šå†—ä½™å­—æ®µã€‚"""
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        raise ConfigError(f"æœªæ‰¾åˆ° base_folder: {base_dir}")
    keys_map: Dict[str, set] = {}
    for fp in sorted(base_dir.glob("*.strings")):
        _, entries = parse_strings_file(fp)
        keys_map[fp.name] = set(e.key for e in entries)
    return keys_map


def scan_redundant_keys(cfg: StringsI18nConfig, base_keys_map: Dict[str, set]) -> Dict[str, List[str]]:
    """å†—ä½™å­—æ®µï¼šBase ä¸­æ²¡æœ‰ï¼Œä½†å…¶ä»–è¯­è¨€ä¸­æœ‰çš„ keyã€‚è¿”å› {locale_code: ["File.strings:key", ...]}"""
    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    report: Dict[str, List[str]] = {}
    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue
        redundant: List[str] = []
        for fp in sorted(loc_dir.glob("*.strings")):
            base_keys = base_keys_map.get(fp.name, set())
            _, entries = parse_strings_file(fp)
            for e in entries:
                if e.key not in base_keys:
                    redundant.append(f"{fp.name}:{e.key}")
        if redundant:
            redundant = sorted(set(redundant), key=lambda s: (s.split(":",1)[0], s.split(":",1)[1]))
            report[loc.code] = redundant
    return report


def _format_key_report(report: Dict[str, List[str]], *, title: str, max_keys_per_file: int = 30) -> str:
    lines: List[str] = []
    lines.append(title)
    lines.append("")
    for lang, items in sorted(report.items(), key=lambda kv: kv[0]):
        by_file: Dict[str, List[str]] = {}
        for it in items:
            if ":" in it:
                fn, key = it.split(":", 1)
            else:
                fn, key = "(unknown)", it
            by_file.setdefault(fn, []).append(key)

        total = sum(len(v) for v in by_file.values())
        lines.append(f"ã€{lang}ã€‘å…± {total} ä¸ª")
        for fn in sorted(by_file.keys()):
            keys = sorted(set(by_file[fn]))
            shown = keys[:max_keys_per_file]
            remain = len(keys) - len(shown)
            preview = ", ".join(shown)
            if remain > 0:
                preview = preview + f", â€¦ï¼ˆè¿˜æœ‰ {remain} ä¸ªï¼‰"
            wrapped = textwrap.fill(preview, width=100, subsequent_indent=" " * (len(fn) + 6))
            lines.append(f"  - {fn} ({len(keys)}): {wrapped}")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def _write_report_file(cfg: StringsI18nConfig, content: str, *, name: str) -> Optional[Path]:
    """doctor é˜¶æ®µä¸è½ç›˜æŠ¥å‘Šï¼šæŒ‰éœ€æ±‚ç¦ç”¨æ‰€æœ‰ report æ–‡ä»¶å†™å…¥ã€‚"""
    return None


def _resolve_placeholder_mismatch_policy(
    cfg: StringsI18nConfig,
    mismatches: Dict[str, Dict[str, List[Tuple[str, List[str], List[str]]]]],
    *,
    max_items: int = 3,
) -> str:
    if not mismatches:
        return "keep"

    flat: List[Tuple[str, str, str, List[str], List[str]]] = []
    for lang, by_file in mismatches.items():
        for fn, items in by_file.items():
            for (k, bph, tph) in items:
                flat.append((lang, fn, k, bph, tph))
    flat.sort(key=lambda x: (x[0], x[1], x[2]))

    lines: List[str] = []
    lines.append("âš ï¸ å‘ç°å ä½ç¬¦ä¸ä¸€è‡´ï¼ˆç¤ºä¾‹ï¼‰ï¼š")
    for i, (lang, fn, k, bph, tph) in enumerate(flat[:max_items], 1):
        lines.append(f"  {i}. {lang}/{fn}  key={k}")
        lines.append(f"     Base: {bph}")
        lines.append(f"     Lang: {tph}")
    if len(flat) > max_items:
        lines.append(f"  â€¦ï¼ˆè¿˜æœ‰ {len(flat) - max_items} æ¡æœªå±•ç¤ºï¼‰")
    lines.append("")
    lines.append("ä¿®å¤å»ºè®®ï¼š")
    lines.append("  - æ¨èï¼šåˆ é™¤è¿™äº›æ¡ç›®ï¼Œè®© translate æŒ‰ Base é‡æ–°ç”Ÿæˆï¼ˆæœ€å®‰å…¨ï¼Œé¿å…è¿è¡Œæ—¶å´©æºƒï¼‰")
    lines.append("  - æˆ–è€…ï¼šäººå·¥ä¿®æ­£ç›®æ ‡è¯­è¨€ value çš„å ä½ç¬¦ï¼Œä½¿å…¶ä¸ Base å®Œå…¨ä¸€è‡´")
    content = "\n".join(lines) + "\n"
    print(content)
    p = _write_report_file(cfg, content, name="placeholder_mismatch_preview")
    if p is not None:
        print(f"ğŸ“„ å·²è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ï¼š{p}")

    opt = (cfg.options or {}).get("placeholder_mismatch_policy")
    if opt in {"keep", "delete"}:
        print(f"âœ… ä½¿ç”¨é…ç½® placeholder_mismatch_policy={opt}")
        return opt

    if not sys.stdin.isatty():
        return "keep"

    while True:
        ans = input("å¦‚ä½•å¤„ç†å ä½ç¬¦ä¸ä¸€è‡´ï¼Ÿ(d=åˆ é™¤è¿™äº›æ¡ç›® / k=ä¿ç•™ç»§ç»­) [k]: ").strip().lower()
        if ans == "" or ans == "k":
            return "keep"
        if ans == "d":
            return "delete"
        print("è¯·è¾“å…¥ d æˆ– k")


def _apply_placeholder_mismatch_delete(
    cfg: StringsI18nConfig,
    mismatches: Dict[str, Dict[str, List[Tuple[str, List[str], List[str]]]]],
) -> int:
    deleted = 0
    for lang, by_file in mismatches.items():
        loc_dir = (cfg.lang_root / f"{lang}.lproj").resolve()
        if not loc_dir.exists():
            continue
        for fn, items in by_file.items():
            fp = (loc_dir / fn).resolve()
            if not fp.exists():
                continue
            try:
                preamble, entries = parse_strings_file(fp)
            except Exception:
                continue
            bad_keys = {k for (k, _, _) in items}
            if not bad_keys:
                continue
            new_entries = [e for e in entries if e.key not in bad_keys]
            if len(new_entries) != len(entries):
                deleted += (len(entries) - len(new_entries))
                write_strings_file(fp, preamble, new_entries, group_by_prefix=False)
    return deleted


def _resolve_redundant_policy(cfg: StringsI18nConfig, report: Dict[str, List[str]]) -> str:
    if not report:
        return "keep"

    content = _format_key_report(report, title="âš ï¸ å‘ç°å†—ä½™å­—æ®µï¼ˆBase ä¸­æ²¡æœ‰ï¼Œä½†å…¶ä»–è¯­è¨€å­˜åœ¨ï¼‰ï¼š", max_keys_per_file=4)
    print(content)
    p = _write_report_file(cfg, content, name="redundant_keys")
    if p is not None:
        print(f"ğŸ“„ å·²è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ï¼š{p}")

    opt = (cfg.options or {}).get("redundant_key_policy")
    if opt in {"keep", "delete"}:
        print(f"âœ… ä½¿ç”¨é…ç½® redundant_key_policy={opt}")
        return opt

    while True:
        ans = input("æ˜¯å¦åˆ é™¤è¿™äº›å†—ä½™å­—æ®µï¼Ÿ(y=åˆ é™¤ / n=ä¿ç•™ / c=å–æ¶ˆæœ¬æ¬¡ sort) [n]: ").strip().lower()
        if ans == "" or ans == "n":
            return "keep"
        if ans == "y":
            return "delete"
        if ans == "c":
            return "cancel"
        print("è¯·è¾“å…¥ y / n / c")


def scan_duplicate_keys(cfg: StringsI18nConfig) -> Dict[str, List[str]]:
    result: Dict[str, set] = {}

    def add(lang_label: str, keys: List[str]) -> None:
        if not keys:
            return
        s = result.get(lang_label)
        if s is None:
            s = set()
            result[lang_label] = s
        s.update(keys)

    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if base_dir.exists():
        for fp in sorted(base_dir.glob("*.strings")):
            _, entries = parse_strings_file(fp)
            add("Base", _collect_duplicates(entries))

    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue
        for fp in sorted(loc_dir.glob("*.strings")):
            _, entries = parse_strings_file(fp)
            add(loc.code, _collect_duplicates(entries))

    return {k: sorted(list(v)) for k, v in result.items()}


def _resolve_duplicate_policy(cfg: StringsI18nConfig, dup_report: Dict[str, List[str]]) -> str:
    if not dup_report:
        return "keep_first"

    opt = (cfg.options or {}).get("duplicate_key_policy")
    if isinstance(opt, str) and opt in {"keep_first", "delete_all"}:
        return opt

    print("\nâš ï¸ æ£€æµ‹åˆ°é‡å¤ keyï¼š")
    for lang, keys in dup_report.items():
        print(f"- {lang}: {keys}")
    print("\nè¯·é€‰æ‹©å¤„ç†ç­–ç•¥ï¼š\n  1) åªä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆkeep_firstï¼‰\n  2) å…¨éƒ¨åˆ é™¤ï¼ˆdelete_allï¼‰\n  3) å–æ¶ˆæœ¬æ¬¡ sort\n")
    try:
        choice = input("è¾“å…¥ 1/2/3ï¼ˆé»˜è®¤ 1ï¼‰ï¼š").strip()
    except EOFError:
        choice = ""

    if choice == "2":
        return "delete_all"
    if choice == "3":
        return "cancel"
    return "keep_first"


def sort_base_strings_files(cfg: StringsI18nConfig, *, duplicate_policy: str) -> int:
    base_dir = cfg.lang_root / cfg.base_folder
    if not base_dir.exists():
        raise ConfigError(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")

    files = sorted(base_dir.glob("*.strings"))
    if not files:
        print(f"âš ï¸ Base.lproj ä¸‹æœªæ‰¾åˆ° *.stringsï¼š{base_dir}")
        return 0

    changed = 0
    for fp in files:
        preamble, entries = parse_strings_file(fp)

        entries = _apply_duplicate_policy(entries, duplicate_policy)
        _, entries_sorted = sort_strings_entries(preamble, entries)

        old_text = fp.read_text(encoding="utf-8") if fp.exists() else ""
        tmp_path = fp.with_suffix(fp.suffix + ".__tmp__")
        write_strings_file(tmp_path, preamble, entries_sorted, group_by_prefix=True)
        new_text = tmp_path.read_text(encoding="utf-8")
        tmp_path.unlink(missing_ok=True)

        if old_text != new_text:
            fp.write_text(new_text, encoding="utf-8")
            changed += 1

    return changed


def sort_other_locale_strings_files(cfg: StringsI18nConfig, *, duplicate_policy: str, base_keys_map: Dict[str, set], redundant_policy: str) -> int:
    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    changed = 0
    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue

        files = sorted(loc_dir.glob("*.strings"))
        for fp in files:
            preamble, entries = parse_strings_file(fp)

            entries = _apply_duplicate_policy(entries, duplicate_policy)

            if redundant_policy == "delete":
                base_keys = base_keys_map.get(fp.name, set())
                entries = [e for e in entries if e.key in base_keys]

            entries_sorted = sorted(entries, key=lambda e: e.key)

            old_text = fp.read_text(encoding="utf-8") if fp.exists() else ""
            tmp_path = fp.with_suffix(fp.suffix + ".__tmp__")
            write_strings_file(tmp_path, preamble, entries_sorted, group_by_prefix=False)
            new_text = tmp_path.read_text(encoding="utf-8")
            tmp_path.unlink(missing_ok=True)

            if old_text != new_text:
                fp.write_text(new_text, encoding="utf-8")
                changed += 1

    return changed


def _scan_base_camelcase_conflicts_for_sort(cfg: StringsI18nConfig) -> Dict[str, Dict[str, List[str]]]:
    """ç»™ sort ç”¨ï¼šæŒ‰ Base æ–‡ä»¶æ‰«æ camelCase å†²çªï¼Œè¿”å› {filename: {prop: [keys...]}}"""
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    out: Dict[str, Dict[str, List[str]]] = {}
    if not base_dir.exists():
        return out
    for fp in sorted(base_dir.glob("*.strings")):
        try:
            _, entries = parse_strings_file(fp)
        except Exception:
            continue
        conflicts = scan_camelcase_conflicts(entries)
        if conflicts:
            out[fp.name] = conflicts
    return out


def run_sort(cfg: StringsI18nConfig) -> None:
    # sort ä¹‹å‰éœ€è¦å…ˆæ£€æµ‹æ–‡ä»¶å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªè¯­è¨€ç›®å½•ä¸‹çš„ *.strings ä¸ Base.lproj ä¸€è‡´
    if run_doctor(cfg) != 0:
        print("âŒ sort ä¸­æ­¢ï¼šdoctor æœªé€šè¿‡")
        return

    # âœ… å†é¢å¤–â€œæ˜ç¡®æ‰“å°ä¸€æ¬¡â€camelCase å†²çªï¼ˆæ»¡è¶³ä½ â€œsort æ£€æŸ¥æ—¶ä¹Ÿè¦æ‰“å°å‡ºæ¥å¤„ç†â€çš„è¦æ±‚ï¼‰
    camel_conflicts_by_file = _scan_base_camelcase_conflicts_for_sort(cfg)
    if camel_conflicts_by_file:
        print("\nâŒ sort æ£€æµ‹åˆ° Base Swift camelCase å±æ€§åå†²çªï¼ˆè¯·å…ˆæ‰‹åŠ¨å¤„ç† keyï¼›sort ä¸ä¼šè‡ªåŠ¨ä¿®ï¼‰ï¼š")
        for fn in sorted(camel_conflicts_by_file.keys()):
            conflicts = camel_conflicts_by_file[fn]
            print(f"\n[{fn}]")
            for prop in sorted(conflicts.keys()):
                print(f"- {prop}: {conflicts[prop]}")
        return

    try:
        created_dirs, created_files = ensure_strings_files_integrity(cfg)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    if created_dirs or created_files:
        print(f"âœ… å®Œæ•´æ€§ä¿®å¤ï¼šåˆ›å»ºç›®å½• {created_dirs} ä¸ªï¼Œåˆ›å»º .strings æ–‡ä»¶ {created_files} ä¸ª")
    else:
        print("âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼šå„è¯­è¨€ *.strings æ–‡ä»¶é›†ä¸ Base ä¸€è‡´")

    dup_report = scan_duplicate_keys(cfg)
    policy = _resolve_duplicate_policy(cfg, dup_report)
    if policy == "cancel":
        print("âŒ sort å·²å–æ¶ˆï¼ˆæœªåšä»»ä½•ä¿®æ”¹ï¼‰")
        return

    try:
        base_keys_map = _base_keys_by_file(cfg)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    redundant_report = scan_redundant_keys(cfg, base_keys_map)
    redundant_policy = _resolve_redundant_policy(cfg, redundant_report)
    if redundant_policy == "cancel":
        print("âŒ sort å·²å–æ¶ˆï¼ˆæœªåšä»»ä½•ä¿®æ”¹ï¼‰")
        return

    try:
        base_changed = sort_base_strings_files(cfg, duplicate_policy=policy)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    try:
        other_changed = sort_other_locale_strings_files(
            cfg,
            duplicate_policy=policy,
            base_keys_map=base_keys_map,
            redundant_policy=redundant_policy,
        )
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    if base_changed:
        print(f"âœ… Base.lproj æ’åºå®Œæˆï¼šæ›´æ–° {base_changed} ä¸ª .strings æ–‡ä»¶")
    else:
        print("âœ… Base.lproj å·²æ˜¯æœ‰åºçŠ¶æ€ï¼šæ— éœ€æ”¹åŠ¨")

    if other_changed:
        print(f"âœ… å…¶ä»–è¯­è¨€æ’åºå®Œæˆï¼šæ›´æ–° {other_changed} ä¸ª .strings æ–‡ä»¶")
    else:
        print("âœ… å…¶ä»–è¯­è¨€å·²æ˜¯æœ‰åºçŠ¶æ€ï¼šæ— éœ€æ”¹åŠ¨")
