from __future__ import annotations

import json
import re
import datetime
import textwrap
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import yaml

# ----------------------------
# å¸¸é‡ / é»˜è®¤æ–‡ä»¶å
# ----------------------------
DEFAULT_TEMPLATE_NAME = "strings_i18n.yaml"     # å†…ç½®æ¨¡æ¿æ–‡ä»¶ï¼ˆå¸¦æ³¨é‡Šï¼‰
DEFAULT_LANGUAGES_NAME = "languages.json"      # æœ¬åœ°è¯­è¨€åˆ—è¡¨æ–‡ä»¶ï¼ˆcode + name_enï¼‰


# ----------------------------
# å¼‚å¸¸ç±»å‹
# ----------------------------
class ConfigError(RuntimeError):
    """ç”¨äºå¯åŠ¨é˜¶æ®µçš„é…ç½®é”™è¯¯ï¼ˆæ›´å‹å¥½çš„æŠ¥é”™ä¸è§£å†³å»ºè®®ï¼‰"""
    pass


# ----------------------------
# æ•°æ®æ¨¡å‹ï¼ˆæŒ‰ strings_i18n.yaml schemaï¼‰
# ----------------------------
@dataclass(frozen=True)
class Locale:
    code: str
    name_en: str


@dataclass(frozen=True)
class StringsI18nConfig:
    # è·¯å¾„
    project_root: Path
    languages_path: Path          # ç»å¯¹è·¯å¾„
    lang_root: Path               # ç»å¯¹è·¯å¾„ï¼š*.lproj æ‰€åœ¨ç›®å½•
    base_folder: str              # e.g. Base.lproj

    # è¯­è¨€
    base_locale: Locale
    source_locale: Locale
    core_locales: List[Locale]
    target_locales: List[Locale]

    # è¡Œä¸ºå¼€å…³
    options: Dict[str, Any]
    prompts: Dict[str, Any]


# ----------------------------
# å†…ç½®æ–‡ä»¶è¯»å–ï¼ˆæ¨¡æ¿ / é»˜è®¤ languages.jsonï¼‰
# ----------------------------
def _pkg_file(name: str) -> Path:
    # é»˜è®¤æŠŠæ¨¡æ¿ä¸é»˜è®¤ languages.json æ”¾åœ¨ data.py åŒç›®å½•
    return Path(__file__).with_name(name)


def ensure_languages_json(project_root: Path, languages_rel: str = DEFAULT_LANGUAGES_NAME) -> Path:
    """å¦‚æœæœ¬åœ°æ²¡æœ‰ languages.jsonï¼Œåˆ™ç”¨å†…ç½®é»˜è®¤ languages.json ç”Ÿæˆä¸€ä»½ã€‚"""
    project_root = project_root.resolve()
    dst = (project_root / languages_rel).resolve()

    if dst.exists():
        return dst

    src = _pkg_file(DEFAULT_LANGUAGES_NAME)
    if not src.exists():
        raise FileNotFoundError(f"å†…ç½®é»˜è®¤ {DEFAULT_LANGUAGES_NAME} ä¸å­˜åœ¨ï¼š{src}")

    dst.parent.mkdir(parents=True, exist_ok=True)
    dst.write_text(src.read_text(encoding="utf-8"), encoding="utf-8")
    return dst


def _load_languages(languages_path: Path) -> List[Dict[str, str]]:
    arr = json.loads(languages_path.read_text(encoding="utf-8"))
    if not isinstance(arr, list):
        raise ValueError(f"{languages_path.name} é¡¶å±‚å¿…é¡»æ˜¯æ•°ç»„")
    out: List[Dict[str, str]] = []
    for item in arr:
        if not isinstance(item, dict):
            continue
        code = str(item.get("code", "")).strip()
        name_en = str(item.get("name_en", "")).strip()
        if not code or not name_en:
            continue
        out.append({"code": code, "name_en": name_en})
    return out




def _all_locale_codes(cfg: StringsI18nConfig) -> List[str]:
    codes: List[str] = []
    for loc in [cfg.base_locale, cfg.source_locale] + cfg.core_locales + cfg.target_locales:
        if loc and loc.code not in codes:
            codes.append(loc.code)
    return codes


def _dedup_locales_preserve_order(locales: List[Locale]) -> List[Locale]:
    seen: set[str] = set()
    out: List[Locale] = []
    for l in locales:
        if l.code in seen:
            continue
        seen.add(l.code)
        out.append(l)
    return out


_PRINTF_RE = re.compile(r'%(?:\d+\$)?(?:@|d|i|u|f|s|ld|lld|lu|llu|lf)', re.IGNORECASE)

def _extract_printf_placeholders(value: str) -> List[str]:
    # å¿½ç•¥è½¬ä¹‰çš„ %%ï¼ˆå®ƒä¸æ˜¯å ä½ç¬¦ï¼‰
    if not value:
        return []
    # ä¸´æ—¶æ›¿æ¢ %% é˜²æ­¢è¢«æ­£åˆ™è¯¯ä¼¤
    tmp = value.replace("%%", "")
    return _PRINTF_RE.findall(tmp)


def _doctor_print_and_write(
    cfg: StringsI18nConfig,
    errors: List[str],
    warns: List[str],
    extra_sections: Optional[Dict[str, Any]] = None,
) -> int:
    # æ§åˆ¶å°æ‘˜è¦
    print("\n=== doctor summary ===")
    print(f"- project_root: {cfg.project_root}")
    print(f"- lang_root:    {cfg.lang_root}")
    print(f"- base_folder:  {cfg.base_folder}")
    print(f"- base_locale:  {cfg.base_locale.code}")
    print(f"- source_locale:{cfg.source_locale.code}")
    print(f"- core_locales: {[l.code for l in cfg.core_locales]}")
    print(f"- target_locales: {len(cfg.target_locales)}")

    if errors:
        print("\n[ERROR]")
        for e in errors:
            print(f"- {e}")
    if warns:
        print("\n[WARN]")
        for w in warns:
            print(f"- {w}")

    # å†™æŠ¥å‘Šæ–‡ä»¶ï¼ˆå«è¯¦ç»† sectionï¼‰
    try:
        lines: List[str] = []
        lines.append("box_strings_i18n doctor report")
        lines.append("")
        lines.append("=== summary ===")
        lines.append(f"project_root: {cfg.project_root}")
        lines.append(f"lang_root:    {cfg.lang_root}")
        lines.append(f"base_folder:  {cfg.base_folder}")
        lines.append(f"base_locale:  {cfg.base_locale.code}")
        lines.append(f"source_locale:{cfg.source_locale.code}")
        lines.append(f"core_locales: {[l.code for l in cfg.core_locales]}")
        lines.append(f"target_locales: {len(cfg.target_locales)}")

        if errors:
            lines.append("")
            lines.append("[ERROR]")
            for e in errors:
                lines.append(f"- {e}")
        if warns:
            lines.append("")
            lines.append("[WARN]")
            for w in warns:
                lines.append(f"- {w}")

        if extra_sections:
            lines.append("")
            lines.append("=== details ===")
            for k, v in (extra_sections or {}).items():
                lines.append("")
                lines.append(f"## {k}")
                if isinstance(v, str):
                    lines.append(v.rstrip())
                else:
                    lines.append(pprint.pformat(v, width=120))

        content = "\n".join(lines).rstrip() + "\n"
        report_path = _write_report_file(cfg, content, name="doctor")
        if report_path is not None:
            print(f"\nReport: {report_path}")
    except Exception as e:
        print(f"\nReport å†™å…¥å¤±è´¥ï¼š{e}")

    return 1 if errors else 0
def build_target_locales_from_languages_json(
    languages_path: Path,
    *,
    source_code: str,
    core_codes: List[str],
) -> Tuple[List[Dict[str, str]], int]:
    """
    ä» languages.json ç”Ÿæˆ target_localesï¼ˆcode + name_enï¼‰ï¼Œå¹¶ï¼š
    - æŒ‰ code å»é‡ï¼ˆä¿åºï¼‰
    - å‰”é™¤ source_code
    - å‰”é™¤ core_codes
    è¿”å›ï¼š(targets, removed_count)
    """
    items = _load_languages(languages_path)
    seen = set()
    out: List[Dict[str, str]] = []
    removed = 0

    core_set = set(core_codes)

    for it in items:
        code = it["code"]
        if code == source_code or code in core_set:
            removed += 1
            continue
        if code in seen:
            continue
        seen.add(code)
        out.append(it)

    return out, removed


# ----------------------------
# YAML æ¨¡æ¿â€œä¿æ³¨é‡Šâ€å±€éƒ¨æ›¿æ¢ï¼štarget_locales block
# ----------------------------
def _yaml_block_for_target_locales(locales: List[Dict[str, str]]) -> str:
    lines = ["target_locales:"]
    for it in locales:
        lines.append(f"  - code: {it['code']}")
        lines.append(f"    name_en: {it['name_en']}")
    return "\n".join(lines) + "\n"


def replace_target_locales_block(template_text: str, new_locales: List[Dict[str, str]]) -> str:
    """
    ä»…æ›¿æ¢æ¨¡æ¿ä¸­ `target_locales:` æ®µè½çš„å†…å®¹ï¼Œå…¶ä»–æ³¨é‡Š/æ’ç‰ˆä¿ç•™ã€‚
    åŒ¹é…è§„åˆ™ï¼šä» `target_locales:` å¼€å§‹ï¼Œæ›¿æ¢åˆ°ä¸‹ä¸€ä¸ªé¡¶å±‚ key ä¹‹å‰ã€‚
    """
    new_block = _yaml_block_for_target_locales(new_locales)

    start_match = re.search(r"(?m)^target_locales:\s*$", template_text)
    if not start_match:
        raise ValueError("æ¨¡æ¿ä¸­æœªæ‰¾åˆ° target_locales: æ®µè½")

    start = start_match.start()
    after = template_text[start_match.end():]

    # ä¸‹ä¸€æ®µé¡¶å±‚ keyï¼ˆå½¢å¦‚ prompts:, options:, languages: ç­‰ï¼‰
    next_key = re.search(r"(?m)^(?!target_locales:)[A-Za-z_][A-Za-z0-9_]*:\s*$", after)

    if next_key:
        end = start_match.end() + next_key.start()
    else:
        end = len(template_text)

    return template_text[:start] + new_block + template_text[end:]


# ----------------------------
# initï¼šç”Ÿæˆ/æ ¡éªŒé…ç½®ï¼Œç¡®ä¿ languages.json + lang_root å­˜åœ¨
# ----------------------------
def init_config(project_root: Path, cfg_path: Path) -> None:
    project_root = project_root.resolve()
    cfg_path = cfg_path.resolve()

    # 1) cfg ä¸å­˜åœ¨ï¼šç”¨å†…ç½®æ¨¡æ¿ç”Ÿæˆï¼ˆä¿ç•™æ³¨é‡Šï¼‰+ åŠ¨æ€æ›¿æ¢ target_locales
    if not cfg_path.exists():
        tpl = _pkg_file(DEFAULT_TEMPLATE_NAME)
        if not tpl.exists():
            raise FileNotFoundError(f"å†…ç½®é»˜è®¤é…ç½®æ¨¡æ¿ä¸å­˜åœ¨ï¼š{tpl}")

        tpl_text = tpl.read_text(encoding="utf-8")
        raw_tpl = yaml.safe_load(tpl_text) or {}
        validate_config(raw_tpl)  # æ¨¡æ¿è‡ªèº«ä¹Ÿè¦åˆæ³•

        # 2) å…ˆç¡®ä¿ languages.json å­˜åœ¨ï¼ˆæŒ‰æ¨¡æ¿é‡Œçš„ languages å­—æ®µï¼‰
        languages_rel = str(raw_tpl.get("languages") or DEFAULT_LANGUAGES_NAME)
        languages_path = ensure_languages_json(project_root, languages_rel=languages_rel)

        # 3) ç”Ÿæˆ targetsï¼šlanguages - core - source
        src = _first_locale(raw_tpl["source_locale"])
        core = [_locale_obj(x) for x in (raw_tpl.get("core_locales") or [])]
        targets, _removed = build_target_locales_from_languages_json(
            languages_path,
            source_code=src.code,
            core_codes=[c.code for c in core],
        )

        out_text = replace_target_locales_block(tpl_text, targets)
        cfg_path.parent.mkdir(parents=True, exist_ok=True)
        cfg_path.write_text(out_text, encoding="utf-8")

    # 4) æ ¡éªŒé…ç½®ï¼ˆinit é˜¶æ®µä¸å¼ºåˆ¶æ£€æŸ¥ç›®å½•å­˜åœ¨ï¼‰
    assert_config_ok(cfg_path, project_root=project_root, check_paths_exist=False)

    # 5) åˆ›å»º lang_root ç›®å½•ï¼ˆæŒ‰ project_root è§£æï¼‰
    raw = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
    lang_root = (project_root / str(raw["lang_root"])).resolve()
    lang_root.mkdir(parents=True, exist_ok=True)

    # 6) ç¡®ä¿ languages æ–‡ä»¶å­˜åœ¨ï¼ˆæŒ‰é…ç½®ï¼‰
    languages_rel = str(raw.get("languages") or DEFAULT_LANGUAGES_NAME)
    ensure_languages_json(project_root, languages_rel=languages_rel)


# ----------------------------
# å¯åŠ¨ä¼˜å…ˆæ ¡éªŒå…¥å£
# ----------------------------
def assert_config_ok(
    cfg_path: Path,
    *,
    project_root: Optional[Path] = None,
    check_paths_exist: bool = True,
) -> Dict[str, Any]:
    cfg_path = cfg_path.resolve()
    project_root = (project_root or cfg_path.parent).resolve()

    if not cfg_path.exists():
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{cfg_path}\n"
            f"è§£å†³æ–¹æ³•ï¼šè¿è¡Œ `box_strings_i18n init` ç”Ÿæˆé»˜è®¤é…ç½®ã€‚"
        )

    try:
        raw = yaml.safe_load(cfg_path.read_text(encoding="utf-8")) or {}
    except Exception as e:
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶æ— æ³•è§£æä¸º YAMLï¼š{cfg_path}\n"
            f"åŸå› ï¼š{e}\n"
            f"è§£å†³æ–¹æ³•ï¼šä¿®å¤ YAML æ ¼å¼æˆ–è¿è¡Œ `box_strings_i18n init` é‡æ–°ç”Ÿæˆã€‚"
        )

    try:
        validate_config(raw)
    except Exception as e:
        raise ConfigError(
            f"é…ç½®æ–‡ä»¶æ ¡éªŒå¤±è´¥ï¼š{cfg_path}\n"
            f"åŸå› ï¼š{e}\n"
            f"è§£å†³æ–¹æ³•ï¼šä¿®å¤é…ç½®å­—æ®µ/ç±»å‹ï¼Œæˆ–è¿è¡Œ `box_strings_i18n init` é‡æ–°ç”Ÿæˆã€‚"
        )

    if check_paths_exist:
        # languages
        languages_path = (project_root / str(raw["languages"])).resolve()
        if not languages_path.exists():
            raise ConfigError(
                f"languages æ–‡ä»¶ä¸å­˜åœ¨ï¼š{languages_path}\n"
                f"è§£å†³æ–¹æ³•ï¼šè¿è¡Œ `box_strings_i18n init` è‡ªåŠ¨ç”Ÿæˆï¼Œæˆ–ä¿®å¤é…ç½®ä¸­çš„ languages è·¯å¾„ã€‚"
            )

        # lang_root + base_folder
        lang_root = (project_root / str(raw["lang_root"])).resolve()
        if not lang_root.exists():
            raise ConfigError(
                f"lang_root ç›®å½•ä¸å­˜åœ¨ï¼š{lang_root}\n"
                f"è§£å†³æ–¹æ³•ï¼šåˆ›å»ºç›®å½•æˆ–è¿è¡Œ `box_strings_i18n init` è®©å·¥å…·åˆå§‹åŒ–ã€‚"
            )

        base_folder = str(raw["base_folder"])
        base_dir = (lang_root / base_folder).resolve()
        if not base_dir.exists():
            raise ConfigError(
                f"Base è¯­è¨€ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}\n"
                f"è§£å†³æ–¹æ³•ï¼šç¡®è®¤ Xcode å·¥ç¨‹å†… Base.lproj è·¯å¾„ï¼Œæˆ–ä¿®å¤é…ç½®ä¸­çš„ lang_root/base_folderã€‚"
            )

    return raw


# ----------------------------
# load_configï¼šæŠŠ raw dict è½¬æˆ StringsI18nConfigï¼ˆè·¯å¾„è§£æä¸ºç»å¯¹è·¯å¾„ï¼‰
# ----------------------------
def load_config(cfg_path: Path, *, project_root: Optional[Path] = None) -> StringsI18nConfig:
    cfg_path = cfg_path.resolve()
    project_root = (project_root or cfg_path.parent).resolve()

    raw = assert_config_ok(cfg_path, project_root=project_root, check_paths_exist=True)

    languages_path = (project_root / str(raw["languages"])).resolve()
    lang_root = (project_root / str(raw["lang_root"])).resolve()

    base_locale = _first_locale(raw["base_locale"])
    source_locale = _first_locale(raw["source_locale"])
    core_locales = [_locale_obj(x) for x in (raw.get("core_locales") or [])]
    target_locales = [_locale_obj(x) for x in (raw.get("target_locales") or [])]

    return StringsI18nConfig(
        project_root=project_root,
        languages_path=languages_path,
        lang_root=lang_root,
        base_folder=str(raw["base_folder"]),
        base_locale=base_locale,
        source_locale=source_locale,
        core_locales=core_locales,
        target_locales=target_locales,
        options=dict(raw.get("options") or {}),
        prompts=dict(raw.get("prompts") or {}),
    )


# ----------------------------
# validate_configï¼šå­—æ®µ + ç±»å‹ + å…³é”®è¯­ä¹‰æ ¡éªŒ
# ----------------------------
def validate_config(raw: Dict[str, Any]) -> None:
    required_top = [
        "options", "languages", "lang_root", "base_folder",
        "base_locale", "source_locale", "core_locales",
        "target_locales", "prompts",
    ]
    for k in required_top:
        if k not in raw:
            raise ValueError(f"é…ç½®ç¼ºå°‘å­—æ®µï¼š{k}")

    # options
    options = raw["options"]
    if not isinstance(options, dict):
        raise ValueError("options å¿…é¡»æ˜¯ object")

    for k in ["cleanup_extra_keys", "incremental_translate", "normalize_filenames", "sort_keys"]:
        if k not in options:
            raise ValueError(f"options ç¼ºå°‘å­—æ®µï¼š{k}")

    # paths
    for k in ["languages", "lang_root", "base_folder"]:
        if not isinstance(raw[k], str) or not str(raw[k]).strip():
            raise ValueError(f"{k} å¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")

    # locales (è¿™äº›åœ¨æ¨¡æ¿é‡Œæ˜¯ list[object]ï¼Œæ¯ä¸ªåªæ”¾ä¸€ä¸ª)
    _ = _first_locale(raw["base_locale"])
    src = _first_locale(raw["source_locale"])

    core = raw["core_locales"]
    if not isinstance(core, list) or len(core) == 0:
        raise ValueError("core_locales å¿…é¡»æ˜¯éç©ºæ•°ç»„")
    core_locales = [_locale_obj(x) for x in core]

    targets = raw["target_locales"]
    if not isinstance(targets, list):
        raise ValueError("target_locales å¿…é¡»æ˜¯æ•°ç»„ï¼ˆå…è®¸ä¸ºç©ºï¼Œä½†å»ºè®®ç”± init ç”Ÿæˆï¼‰")
    target_locales = [_locale_obj(x) for x in targets]

    # è¯­ä¹‰ï¼šå»é‡ä¸å†²çª
    def codes(locales: List[Locale]) -> List[str]:
        return [x.code for x in locales]

    core_codes = codes(core_locales)
    if len(set(core_codes)) != len(core_codes):
        raise ValueError("core_locales.code å­˜åœ¨é‡å¤ï¼Œè¯·å»é‡")

    tgt_codes = codes(target_locales)
    if len(set(tgt_codes)) != len(tgt_codes):
        raise ValueError("target_locales.code å­˜åœ¨é‡å¤ï¼Œè¯·å»é‡")

    if src.code in set(tgt_codes):
        raise ValueError("target_locales é‡ŒåŒ…å« source_locale.codeï¼Œè¯·ç§»é™¤ï¼ˆsource ä¸èƒ½ä½œä¸º targetï¼‰")

    # prompts
    prompts = raw["prompts"]
    if not isinstance(prompts, dict):
        raise ValueError("prompts å¿…é¡»æ˜¯ object")
    if "default_en" not in prompts or not isinstance(prompts["default_en"], str):
        raise ValueError("prompts.default_en å¿…é¡»å­˜åœ¨ä¸”ä¸ºå­—ç¬¦ä¸²")


def _locale_obj(obj: Any) -> Locale:
    if not isinstance(obj, dict):
        raise ValueError("locale item å¿…é¡»æ˜¯ object")
    code = str(obj.get("code", "")).strip()
    name_en = str(obj.get("name_en", "")).strip()
    if not code or not name_en:
        raise ValueError("locale.code/name_en ä¸èƒ½ä¸ºç©º")
    return Locale(code=code, name_en=name_en)


def _first_locale(obj: Any) -> Locale:
    if not isinstance(obj, list) or len(obj) == 0:
        raise ValueError("locale å¿…é¡»æ˜¯éç©ºæ•°ç»„ï¼ˆlistï¼‰ï¼Œä¸”ç¬¬ä¸€é¡¹ä¸º object")
    return _locale_obj(obj[0])


# ----------------------------
# commandsï¼šdoctor/sortï¼ˆéª¨æ¶ï¼‰
# ----------------------------
def run_doctor(cfg: StringsI18nConfig) -> int:
    """
    æœ€ä½³å®è·µçš„ doctorï¼š
    - é…ç½® & ç›®å½•ç»“æ„æ ¡éªŒ
    - Base.lproj/å…¶å®ƒè¯­è¨€ *.strings å¯è§£ææ€§æ£€æŸ¥
    - key ä¸€è‡´æ€§ï¼ˆç¼ºå¤±/å†—ä½™ï¼‰ç»Ÿè®¡
    - é‡å¤ key æ£€æµ‹ï¼ˆBase è§†ä¸ºé”™è¯¯ï¼›å…¶å®ƒè¯­è¨€è§†ä¸ºè­¦å‘Šï¼‰
    - printf å ä½ç¬¦ä¸€è‡´æ€§ï¼ˆ%@/%d/%1$@ ...ï¼‰æ£€æŸ¥ï¼ˆè­¦å‘Šï¼‰
    è¾“å‡ºï¼š
    - æ§åˆ¶å°å¯è¯»æ‘˜è¦
    - è¯¦ç»†æŠ¥å‘Šå†™å…¥ <lang_root>/.box_strings_i18n_reports/doctor_YYYYMMDD-HHMMSS.txt
    """
    errors: List[str] = []
    warns: List[str] = []

    # ---- è·¯å¾„/ç»“æ„ ----
    if not cfg.lang_root.exists():
        errors.append(f"lang_root ä¸å­˜åœ¨ï¼š{cfg.lang_root}")
        return _doctor_print_and_write(cfg, errors, warns)

    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        errors.append(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")
        return _doctor_print_and_write(cfg, errors, warns)

    if not cfg.languages_path.exists():
        errors.append(f"languages.json ä¸å­˜åœ¨ï¼š{cfg.languages_path}ï¼ˆå¯å…ˆæ‰§è¡Œ initï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆæ¨¡æ¿/æ‹·è´é»˜è®¤ languages.jsonï¼‰")
        return _doctor_print_and_write(cfg, errors, warns)

    # ---- languages.json å†…å®¹ ----
    try:
        languages_list = _load_languages(cfg.languages_path)
        languages = {d['code'] for d in languages_list if 'code' in d}
    except Exception as e:
        errors.append(f"languages.json è¯»å–å¤±è´¥ï¼š{cfg.languages_path}ï¼ˆ{e}ï¼‰")
        return _doctor_print_and_write(cfg, errors, warns)

    # é…ç½®é‡Œå‡ºç°çš„æ‰€æœ‰ locale code éƒ½åº”è¯¥åœ¨ languages.json é‡Œï¼ˆå¦åˆ™ init çš„ target_locales ä¹Ÿå®¹æ˜“å¤±çœŸï¼‰
    cfg_codes = _all_locale_codes(cfg)
    missing_in_languages = [c for c in cfg_codes if c not in languages]
    if missing_in_languages:
        warns.append(
            "languages.json ç¼ºå°‘ä»¥ä¸‹ codeï¼ˆå»ºè®®è¡¥å…¨ï¼Œä»¥ä¾¿ init/æ ¡éªŒä¸€è‡´ï¼‰ï¼š"
            + ", ".join(missing_in_languages)
        )

    # ---- Base.lproj æ–‡ä»¶é›† ----
    base_files = sorted([p for p in base_dir.glob("*.strings") if p.is_file()])
    if not base_files:
        errors.append(f"Base ç›®å½•ä¸‹æœªå‘ç°ä»»ä½• *.stringsï¼š{base_dir}")
        return _doctor_print_and_write(cfg, errors, warns)

    # è§£æ Base å¹¶å»ºç«‹â€œé‡‘æ ‡å‡† key é›†åˆâ€
    base_map: Dict[str, List[StringsEntry]] = {}
    base_keys_by_file: Dict[str, set] = {}
    for fp in base_files:
        try:
            preamble, entries = parse_strings_file(fp)
        except Exception as e:
            errors.append(f"Base è§£æå¤±è´¥ï¼š{fp.name}ï¼ˆ{e}ï¼‰")
            continue

        dups = _collect_duplicates(entries)
        if dups:
            errors.append(f"Base å­˜åœ¨é‡å¤ keyï¼š{fp.name} -> {dups}")
        base_map[fp.name] = entries
        base_keys_by_file[fp.name] = {e.key for e in entries}

    # ---- å…¶å®ƒè¯­è¨€æ£€æŸ¥ ----
    other_locales = [cfg.source_locale] + cfg.core_locales + cfg.target_locales
    other_locales = _dedup_locales_preserve_order(other_locales)

    missing_dirs: List[str] = []
    missing_files: List[str] = []
    parse_fail: List[str] = []

    # ç¼ºå¤±/å†—ä½™ç»Ÿè®¡ï¼ˆæŒ‰ è¯­è¨€->æ–‡ä»¶->keysï¼‰
    missing_keys: Dict[str, Dict[str, List[str]]] = {}
    redundant_keys: Dict[str, Dict[str, List[str]]] = {}

    # å ä½ç¬¦ä¸ä¸€è‡´ï¼šæŒ‰ è¯­è¨€->æ–‡ä»¶->[(key, base_ph, loc_ph)]
    placeholder_mismatch: Dict[str, Dict[str, List[Tuple[str, List[str], List[str]]]]] = {}

    for loc in other_locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            missing_dirs.append(loc.code)
            continue

        for bf in base_files:
            target_fp = (loc_dir / bf.name)
            if not target_fp.exists():
                missing_files.append(f"{loc.code}/{bf.name}")
                continue

            try:
                _, loc_entries = parse_strings_file(target_fp)
            except Exception as e:
                parse_fail.append(f"{loc.code}/{bf.name}ï¼ˆ{e}ï¼‰")
                continue

            # é‡å¤ keyï¼šå…¶å®ƒè¯­è¨€ä»…è­¦å‘Šï¼ˆå› ä¸ºå†å²åŸå› å¯èƒ½å­˜åœ¨ï¼Œä½†ä»åº”æ”¶æ•›ï¼‰
            dups = _collect_duplicates(loc_entries)
            if dups:
                warns.append(f"é‡å¤ keyï¼ˆ{loc.code}/{bf.name}ï¼‰ï¼š{dups}")

            base_keys = base_keys_by_file.get(bf.name, set())
            loc_keys = {e.key for e in loc_entries}

            mk = sorted(list(base_keys - loc_keys))
            rk = sorted(list(loc_keys - base_keys))

            if mk:
                missing_keys.setdefault(loc.code, {}).setdefault(bf.name, []).extend(mk)
            if rk:
                redundant_keys.setdefault(loc.code, {}).setdefault(bf.name, []).extend(rk)

            # printf å ä½ç¬¦ä¸€è‡´æ€§ï¼šåªå¯¹åŒ key åšå¯¹æ¯”
            base_entries_by_key = {e.key: e for e in base_map.get(bf.name, [])}
            loc_entries_by_key = {e.key: e for e in loc_entries}
            for k in (base_keys & loc_keys):
                b = base_entries_by_key.get(k)
                t = loc_entries_by_key.get(k)
                if not b or not t:
                    continue
                bph = _extract_printf_placeholders(b.value)
                tph = _extract_printf_placeholders(t.value)
                if bph != tph:
                    placeholder_mismatch.setdefault(loc.code, {}).setdefault(bf.name, []).append((k, bph, tph))

    if missing_dirs:
        warns.append("ç¼ºå°‘è¯­è¨€ç›®å½•ï¼ˆå¯é€šè¿‡ sort è‡ªåŠ¨è¡¥é½ç©ºæ–‡ä»¶å¤¹/æ–‡ä»¶ï¼‰ï¼š"
                     + ", ".join(sorted(set(missing_dirs))))
    if missing_files:
        warns.append("ç¼ºå°‘ *.strings æ–‡ä»¶ï¼ˆå¯é€šè¿‡ sort è‡ªåŠ¨åˆ›å»ºç©ºæ–‡ä»¶ï¼‰ï¼š"
                     + ", ".join(missing_files[:30]) + (" â€¦" if len(missing_files) > 30 else ""))

    if parse_fail:
        errors.append("ä»¥ä¸‹æ–‡ä»¶è§£æå¤±è´¥ï¼ˆè¯·å…ˆä¿®å¤è¯­æ³•/å¼•å·/åˆ†å·ç­‰ï¼‰ï¼š"
                      + "; ".join(parse_fail[:20]) + (" â€¦" if len(parse_fail) > 20 else ""))

    # ---- æ‘˜è¦æ€§å»ºè®® ----
    # ç¼ºå¤± keyï¼ˆç¿»è¯‘æœªè¦†ç›–ï¼‰åªåšæç¤ºï¼šè¿™æ˜¯æœ€å¸¸è§çš„é—®é¢˜
    miss_count = sum(len(keys) for m in missing_keys.values() for keys in m.values())
    red_count = sum(len(keys) for m in redundant_keys.values() for keys in m.values())
    ph_count = sum(len(v) for m in placeholder_mismatch.values() for v in m.values())

    if miss_count:
        warns.append(f"å‘ç°ç¼ºå¤± keyï¼ˆç›¸å¯¹ Baseï¼‰ï¼šå…± {miss_count} ä¸ªï¼ˆå»ºè®®èµ° translate å¢é‡æˆ–è¡¥é½ï¼‰")
    if red_count:
        warns.append(f"å‘ç°å†—ä½™ keyï¼ˆBase ä¸å­˜åœ¨ï¼‰ï¼šå…± {red_count} ä¸ªï¼ˆå»ºè®®åœ¨ sort ä¸­é€‰æ‹©åˆ é™¤ï¼‰")
    if ph_count:
        warns.append(f"å‘ç°å ä½ç¬¦ä¸ä¸€è‡´ï¼šå…± {ph_count} é¡¹ï¼ˆå»ºè®®äººå·¥ç¡®è®¤ï¼Œé¿å…è¿è¡Œæ—¶å´©æºƒ/æ ¼å¼é”™ä¹±ï¼‰")

    # strict æ¨¡å¼ï¼šæŠŠ warns å½“ errors
    strict = bool(cfg.options.get("doctor_strict", False))
    if strict and warns:
        errors.extend([f"[STRICT] {w}" for w in warns])
        warns = []

    return _doctor_print_and_write(
        cfg,
        errors,
        warns,
        extra_sections={
            "ç¼ºå¤± keyï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": missing_keys,
            "å†—ä½™ keyï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": redundant_keys,
            "å ä½ç¬¦ä¸ä¸€è‡´ï¼ˆæŒ‰è¯­è¨€/æ–‡ä»¶ï¼‰": placeholder_mismatch,
        },
    )


# ----------------------------
# sort å‰çš„å®Œæ•´æ€§æ£€æŸ¥ï¼šç¡®ä¿å„è¯­è¨€ *.lproj ç›®å½•ä¸ Base.lproj çš„ *.strings æ–‡ä»¶é›†ä¸€è‡´
# - è‹¥ç¼ºå¤±ï¼šåˆ›å»ºç›®å½•ä¸ç©ºæ–‡ä»¶
# - è‹¥å¤šä½™ï¼šæš‚ä¸åˆ é™¤ï¼ˆé¿å…è¯¯åˆ é¡¹ç›®è‡ªå®šä¹‰æ–‡ä»¶ï¼‰
# ----------------------------
def ensure_strings_files_integrity(cfg: StringsI18nConfig) -> Tuple[int, int]:
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        raise ConfigError(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")

    base_strings = sorted([p for p in base_dir.glob('*.strings') if p.is_file()])
    if not base_strings:
        # æ²¡æœ‰ä»»ä½• .stringsï¼šè¿™é€šå¸¸æ„å‘³ç€å·¥ç¨‹ç»“æ„ä¸å¯¹æˆ–æœªç”Ÿæˆæœ¬åœ°åŒ–æ–‡ä»¶
        raise ConfigError(
            f"Base ç›®å½•ä¸‹æœªå‘ç°ä»»ä½• .strings æ–‡ä»¶ï¼š{base_dir}\n"
            f"è§£å†³æ–¹æ³•ï¼šç¡®è®¤ Xcode æ˜¯å¦å·²ç”Ÿæˆ Localizable.strings ç­‰æ–‡ä»¶ï¼Œæˆ–æ£€æŸ¥ lang_root/base_folder é…ç½®ã€‚"
        )

    locales: List[Locale] = []
    # source + core + targetï¼ˆBase æœ¬èº«ä¸éœ€è¦å¯¹é½ï¼‰
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    created_dirs = 0
    created_files = 0

    for loc in locales:
        # çº¦å®šï¼š<code>.lprojï¼ˆä¾‹å¦‚ï¼šen.lproj / zh-Hant.lprojï¼‰
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            loc_dir.mkdir(parents=True, exist_ok=True)
            created_dirs += 1

        existing = {p.name for p in loc_dir.glob('*.strings') if p.is_file()}
        for base_file in base_strings:
            if base_file.name not in existing:
                target = loc_dir / base_file.name
                # åˆ›å»ºç©ºæ–‡ä»¶ï¼ˆUTF-8ï¼‰ï¼Œåç»­ translate/sort ä¼šå¡«å……/æ’åº
                target.write_text('', encoding='utf-8')
                created_files += 1

    return created_dirs, created_files



# ----------------------------
# .strings è§£æ/å†™å› + æ’åº
# ----------------------------
_STRINGS_ENTRY_RE = re.compile(r'^\s*"((?:\\.|[^"\\])*)"\s*=\s*"((?:\\.|[^"\\])*)"\s*;\s*$')

@dataclass
class StringsEntry:
    key: str
    value: str
    comments: List[str]  # åŸæ ·ä¿å­˜ï¼ˆè¡Œçº§ï¼‰ï¼Œå†™å›æ—¶æ”¾åœ¨ entry ä¸Šæ–¹


def _is_comment_line(line: str) -> bool:
    s = line.lstrip()
    return s.startswith("//") or s.startswith("/*") or s.startswith("*") or s.startswith("*/")


def _group_prefix(key: str) -> str:
    # è§„åˆ™ï¼šä¼˜å…ˆæŒ‰ '.' çš„ç¬¬ä¸€ä¸ªæ®µï¼›å¦åˆ™æŒ‰ '_' çš„ç¬¬ä¸€ä¸ªæ®µï¼›å¦åˆ™å…¨ key
    if "." in key:
        return key.split(".", 1)[0]
    if "_" in key:
        return key.split("_", 1)[0]
    return key


def parse_strings_file(path: Path) -> Tuple[List[str], List[StringsEntry]]:
    """è§£æ iOS .strings æ–‡ä»¶ï¼Œä¿ç•™æ³¨é‡Šï¼ˆæ³¨é‡Šå½’å±åˆ°å…¶ä¸‹æ–¹çš„ keyï¼‰ã€‚"""
    if not path.exists():
        return [], []

    lines = path.read_text(encoding="utf-8").splitlines()
    preamble: List[str] = []
    entries: List[StringsEntry] = []

    pending_comments: List[str] = []
    seen_first_entry = False

    for line in lines:
        m = _STRINGS_ENTRY_RE.match(line)
        if m:
            key, value = m.group(1), m.group(2)

            # æ¸…ç† commentsï¼šå»æ‰æœ«å°¾å¤šä½™ç©ºè¡Œï¼Œç¡®ä¿â€œæ³¨é‡Šåœ¨å­—æ®µä¸Šæ–¹â€
            while pending_comments and pending_comments[-1].strip() == "":
                pending_comments.pop()

            entries.append(StringsEntry(key=key, value=value, comments=pending_comments))
            pending_comments = []
            seen_first_entry = True
            continue

        if not seen_first_entry:
            # æ–‡ä»¶å¤´éƒ¨ï¼šå®Œæ•´ä¿ç•™ï¼ˆé€šå¸¸æ˜¯ç‰ˆæƒ/è¯´æ˜æ³¨é‡Šï¼‰
            preamble.append(line)
            continue

        # entry ä¹‹é—´çš„å†…å®¹ï¼šè®¤ä¸ºæ˜¯â€œä¸‹ä¸€ä¸ª entry çš„æ³¨é‡Š/ç©ºè¡Œâ€
        if line.strip() == "" or _is_comment_line(line):
            pending_comments.append(line)
        else:
            # å…¼å®¹éæ ‡å‡†è¡Œï¼šä¸ä¸¢å†…å®¹ï¼Œå½’åˆ°ä¸‹ä¸€ä¸ª entry çš„æ³¨é‡Šå—ä¸­
            pending_comments.append(line)

    return preamble, entries


def write_strings_file(path: Path, preamble: List[str], entries: List[StringsEntry], *, group_by_prefix: bool = True) -> None:
    out_lines: List[str] = []

    # å†™ header/preambleï¼ˆåŸæ ·ï¼‰
    if preamble:
        # å»æ‰æœ«å°¾å¤šä½™ç©ºè¡Œï¼ˆé¿å…æ–‡ä»¶å¤´å¤ªæ¾ï¼‰
        while preamble and preamble[-1].strip() == "":
            preamble.pop()
        out_lines.extend(preamble)
        out_lines.append("")  # header ä¸æ­£æ–‡ä¹‹é—´ç•™ä¸€ç©ºè¡Œ

    # entries å·²ç»æ’åº/åˆ†ç»„å®Œæˆï¼›å†™å›æ—¶ä¿è¯ï¼šæ³¨é‡Šç´§è´´åœ¨å­—æ®µä¸Šæ–¹
    prev_group: Optional[str] = None
    for e in entries:
        grp = _group_prefix(e.key)
        if prev_group is not None and grp != prev_group:
            out_lines.append("")  # ç»„ä¹‹é—´ç©ºä¸€è¡Œ
        prev_group = grp

        # å†™æ³¨é‡Š
        if e.comments:
            # å»æ‰æ³¨é‡Šå—é¦–å°¾å¤šä½™ç©ºè¡Œ
            comments = list(e.comments)
            while comments and comments[0].strip() == "":
                comments.pop(0)
            while comments and comments[-1].strip() == "":
                comments.pop()
            out_lines.extend(comments)

        # å†™ entryï¼ˆç»Ÿä¸€æ ¼å¼åŒ–ï¼‰
        out_lines.append(f"\"{e.key}\" = \"{e.value}\";")

    path.write_text("\n".join(out_lines) + "\n", encoding="utf-8")


def sort_strings_entries(preamble: List[str], entries: List[StringsEntry]) -> Tuple[List[str], List[StringsEntry]]:
    # æ ¹æ®å‰ç¼€åˆ†ç»„ + key æ’åº
    entries_sorted = sorted(entries, key=lambda e: (_group_prefix(e.key), e.key))
    return preamble, entries_sorted



def _collect_duplicates(entries: List[StringsEntry]) -> List[str]:
    seen = set()
    dups = set()
    for e in entries:
        if e.key in seen:
            dups.add(e.key)
        else:
            seen.add(e.key)
    return sorted(dups)


def _apply_duplicate_policy(entries: List[StringsEntry], policy: str) -> List[StringsEntry]:
    """å¤„ç†é‡å¤ keyã€‚policy:
    - keep_first: åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„ key
    - delete_all: é‡å¤ keyï¼ˆå‡ºç°>=2ï¼‰å…¨éƒ¨åˆ é™¤
    """
    if policy not in {"keep_first", "delete_all"}:
        return entries

    dups = set(_collect_duplicates(entries))
    if not dups:
        return entries

    if policy == "keep_first":
        kept = []
        seen = set()
        for e in entries:
            if e.key in dups:
                if e.key in seen:
                    continue
                seen.add(e.key)
            kept.append(e)
        return kept

    # delete_all
    return [e for e in entries if e.key not in dups]



def _base_keys_by_file(cfg: StringsI18nConfig) -> Dict[str, set]:
    """è¯»å– Base.lproj ä¸‹æ¯ä¸ª *.strings çš„ key é›†åˆã€‚key ç”¨äºåˆ¤å®šå†—ä½™å­—æ®µã€‚"""
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if not base_dir.exists():
        raise ConfigError(f"æœªæ‰¾åˆ° base_folder: {base_dir}")
    keys_map: Dict[str, set] = {}
    for fp in sorted(base_dir.glob("*.strings")):
        _, entries = parse_strings_file(fp)
        keys_map[fp.name] = set(e.key for e in entries)
    return keys_map


def scan_redundant_keys(cfg: StringsI18nConfig, base_keys_map: Dict[str, set]) -> Dict[str, List[str]]:
    """å†—ä½™å­—æ®µï¼šBase ä¸­æ²¡æœ‰ï¼Œä½†å…¶ä»–è¯­è¨€ä¸­æœ‰çš„ keyã€‚è¿”å› {locale_code: ["File.strings:key", ...]}"""
    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    report: Dict[str, List[str]] = {}
    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue
        redundant: List[str] = []
        for fp in sorted(loc_dir.glob("*.strings")):
            base_keys = base_keys_map.get(fp.name, set())
            _, entries = parse_strings_file(fp)
            for e in entries:
                if e.key not in base_keys:
                    redundant.append(f"{fp.name}:{e.key}")
        if redundant:
            # å»é‡ + æ’åºï¼ˆæŒ‰æ–‡ä»¶åå†æŒ‰ keyï¼‰
            redundant = sorted(set(redundant), key=lambda s: (s.split(":",1)[0], s.split(":",1)[1]))
            report[loc.code] = redundant
    return report



def _format_key_report(report: Dict[str, List[str]], *, title: str, max_keys_per_file: int = 30) -> str:
    """
    å°† {lang: ["File.strings:key", ...]} å˜æˆæ›´æ˜“è¯»çš„æ–‡æœ¬ã€‚
    - è¯­è¨€åˆ†å—
    - æ¯ä¸ªè¯­è¨€æŒ‰æ–‡ä»¶åˆ†ç»„
    - æ¯ä¸ªæ–‡ä»¶æœ€å¤šå±•ç¤º max_keys_per_file ä¸ª keyï¼ˆè¶…å‡ºä¼šæ˜¾ç¤ºâ€œè¿˜æœ‰ N ä¸ªâ€ï¼‰
    """
    lines: List[str] = []
    lines.append(title)
    lines.append("")
    for lang, items in sorted(report.items(), key=lambda kv: kv[0]):
        # group by file
        by_file: Dict[str, List[str]] = {}
        for it in items:
            if ":" in it:
                fn, key = it.split(":", 1)
            else:
                fn, key = "(unknown)", it
            by_file.setdefault(fn, []).append(key)

        total = sum(len(v) for v in by_file.values())
        lines.append(f"ã€{lang}ã€‘å…± {total} ä¸ª")
        for fn in sorted(by_file.keys()):
            keys = sorted(set(by_file[fn]))
            shown = keys[:max_keys_per_file]
            remain = len(keys) - len(shown)
            preview = ", ".join(shown)
            if remain > 0:
                preview = preview + f", â€¦ï¼ˆè¿˜æœ‰ {remain} ä¸ªï¼‰"
            # æ§åˆ¶å•è¡Œå®½åº¦
            wrapped = textwrap.fill(preview, width=100, subsequent_indent=" " * (len(fn) + 6))
            lines.append(f"  - {fn} ({len(keys)}): {wrapped}")
        lines.append("")
    return "\n".join(lines).rstrip() + "\n"


def _write_report_file(cfg: StringsI18nConfig, content: str, *, name: str) -> Optional[Path]:
    """æŠŠæŠ¥å‘Šå†™åˆ° repo å†…çš„ .box_strings_i18n_reports/ï¼Œæ–¹ä¾¿å¤åˆ¶/æŸ¥çœ‹ã€‚"""
    try:
        out_dir = (cfg.lang_root / ".box_strings_i18n_reports").resolve()
        out_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
        out_path = out_dir / f"{name}_{ts}.txt"
        out_path.write_text(content, encoding="utf-8")
        return out_path
    except Exception:
        return None


def _resolve_redundant_policy(cfg: StringsI18nConfig, report: Dict[str, List[str]]) -> str:
    """è¿”å› keep / delete / cancel"""
    if not report:
        return "keep"

    content = _format_key_report(report, title="âš ï¸ å‘ç°å†—ä½™å­—æ®µï¼ˆBase ä¸­æ²¡æœ‰ï¼Œä½†å…¶ä»–è¯­è¨€å­˜åœ¨ï¼‰ï¼š")
    print(content)
    p = _write_report_file(cfg, content, name="redundant_keys")
    if p is not None:
        print(f"ğŸ“„ å·²è¾“å‡ºæŠ¥å‘Šæ–‡ä»¶ï¼š{p}")

    # é…ç½®ä¸­å¯é¢„è®¾ç­–ç•¥ï¼ˆç”¨äº CI/éäº¤äº’ï¼‰ï¼Œå¦åˆ™äº¤äº’è¯¢é—®
    opt = (cfg.options or {}).get("redundant_key_policy")
    if opt in {"keep", "delete"}:
        print(f"âœ… ä½¿ç”¨é…ç½® redundant_key_policy={opt}")
        return opt

    while True:
        ans = input("æ˜¯å¦åˆ é™¤è¿™äº›å†—ä½™å­—æ®µï¼Ÿ(y=åˆ é™¤ / n=ä¿ç•™ / c=å–æ¶ˆæœ¬æ¬¡ sort) [n]: ").strip().lower()
        if ans == "" or ans == "n":
            return "keep"
        if ans == "y":
            return "delete"
        if ans == "c":
            return "cancel"
        print("è¯·è¾“å…¥ y / n / c")


def scan_duplicate_keys(cfg: StringsI18nConfig) -> Dict[str, List[str]]:
    """æ‰«ææ‰€æœ‰è¯­è¨€ï¼ˆå« Baseï¼‰ä¸‹çš„ *.stringsï¼Œè¿”å› {lang_label: [dup_keys...]}"""
    result: Dict[str, set] = {}

    def add(lang_label: str, keys: List[str]) -> None:
        if not keys:
            return
        s = result.get(lang_label)
        if s is None:
            s = set()
            result[lang_label] = s
        s.update(keys)

    # Base
    base_dir = (cfg.lang_root / cfg.base_folder).resolve()
    if base_dir.exists():
        for fp in sorted(base_dir.glob("*.strings")):
            _, entries = parse_strings_file(fp)
            add("Base", _collect_duplicates(entries))

    # other locales
    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue
        for fp in sorted(loc_dir.glob("*.strings")):
            _, entries = parse_strings_file(fp)
            add(loc.code, _collect_duplicates(entries))

    return {k: sorted(list(v)) for k, v in result.items()}


def _resolve_duplicate_policy(cfg: StringsI18nConfig, dup_report: Dict[str, List[str]]) -> str:
    """è‹¥å­˜åœ¨é‡å¤ keyï¼Œå†³å®šå¤„ç†ç­–ç•¥ã€‚ä¼˜å…ˆè¯» cfg.options.duplicate_key_policyã€‚"""
    if not dup_report:
        return "keep_first"

    opt = (cfg.options or {}).get("duplicate_key_policy")
    if isinstance(opt, str) and opt in {"keep_first", "delete_all"}:
        return opt

    # äº¤äº’å¼é€‰æ‹©ï¼ˆè®©ç”¨æˆ·â€œæœ€åå†³å®šâ€ï¼‰
    print("\nâš ï¸ æ£€æµ‹åˆ°é‡å¤ keyï¼š")
    for lang, keys in dup_report.items():
        print(f"- {lang}: {keys}")
    print("\nè¯·é€‰æ‹©å¤„ç†ç­–ç•¥ï¼š\n  1) åªä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆkeep_firstï¼‰\n  2) å…¨éƒ¨åˆ é™¤ï¼ˆdelete_allï¼‰\n  3) å–æ¶ˆæœ¬æ¬¡ sort\n")
    try:
        choice = input("è¾“å…¥ 1/2/3ï¼ˆé»˜è®¤ 1ï¼‰ï¼š").strip()
    except EOFError:
        choice = ""  # éäº¤äº’ç¯å¢ƒ

    if choice == "2":
        return "delete_all"
    if choice == "3":
        return "cancel"
    return "keep_first"


def sort_base_strings_files(cfg: StringsI18nConfig, *, duplicate_policy: str) -> int:
    """å¯¹ Base.lproj ä¸‹çš„æ‰€æœ‰ *.strings æ–‡ä»¶æ’åºï¼ˆä¿ç•™æ³¨é‡Šï¼Œæ³¨é‡Šå†™åœ¨å­—æ®µä¸Šæ–¹ï¼‰ã€‚"""
    base_dir = cfg.lang_root / cfg.base_folder
    if not base_dir.exists():
        raise ConfigError(f"Base ç›®å½•ä¸å­˜åœ¨ï¼š{base_dir}")

    files = sorted(base_dir.glob("*.strings"))
    if not files:
        print(f"âš ï¸ Base.lproj ä¸‹æœªæ‰¾åˆ° *.stringsï¼š{base_dir}")
        return 0

    changed = 0
    for fp in files:
        preamble, entries = parse_strings_file(fp)

        entries = _apply_duplicate_policy(entries, duplicate_policy)
        _, entries_sorted = sort_strings_entries(preamble, entries)

        # æ›´ä¸¥æ ¼ï¼šæ¯”è¾ƒ key åºåˆ— + æ˜¯å¦åˆ†ç»„å†™å›ä¼šæ”¹å˜å†…å®¹
        old_text = fp.read_text(encoding="utf-8") if fp.exists() else ""
        tmp_path = fp.with_suffix(fp.suffix + ".__tmp__")
        write_strings_file(tmp_path, preamble, entries_sorted, group_by_prefix=True)
        new_text = tmp_path.read_text(encoding="utf-8")
        tmp_path.unlink(missing_ok=True)

        if old_text != new_text:
            fp.write_text(new_text, encoding="utf-8")
            changed += 1

    return changed


def sort_other_locale_strings_files(cfg: StringsI18nConfig, *, duplicate_policy: str, base_keys_map: Dict[str, set], redundant_policy: str) -> int:
    """å¯¹é Base è¯­è¨€ç›®å½•ä¸‹çš„æ‰€æœ‰ *.strings æ–‡ä»¶æ’åºï¼ˆä»…æŒ‰ key æ’åºï¼Œä¸åšå‰ç¼€åˆ†ç»„ï¼‰ã€‚"""
    locales: List[Locale] = []
    if cfg.source_locale:
        locales.append(cfg.source_locale)
    locales.extend(cfg.core_locales or [])
    locales.extend(cfg.target_locales or [])

    changed = 0
    for loc in locales:
        loc_dir = (cfg.lang_root / f"{loc.code}.lproj").resolve()
        if not loc_dir.exists():
            continue

        files = sorted(loc_dir.glob("*.strings"))
        for fp in files:
            preamble, entries = parse_strings_file(fp)

            entries = _apply_duplicate_policy(entries, duplicate_policy)

            # å†—ä½™å­—æ®µï¼šBase ä¸­æ²¡æœ‰çš„ keyï¼ˆå¯é€‰åˆ é™¤ï¼‰
            if redundant_policy == "delete":
                base_keys = base_keys_map.get(fp.name, set())
                entries = [e for e in entries if e.key in base_keys]

            entries_sorted = sorted(entries, key=lambda e: e.key)

            old_text = fp.read_text(encoding="utf-8") if fp.exists() else ""
            tmp_path = fp.with_suffix(fp.suffix + ".__tmp__")
            write_strings_file(tmp_path, preamble, entries_sorted, group_by_prefix=False)
            new_text = tmp_path.read_text(encoding="utf-8")
            tmp_path.unlink(missing_ok=True)

            if old_text != new_text:
                fp.write_text(new_text, encoding="utf-8")
                changed += 1

    return changed


def run_sort(cfg: StringsI18nConfig) -> None:
    # sort ä¹‹å‰éœ€è¦å…ˆæ£€æµ‹æ–‡ä»¶å®Œæ•´æ€§ï¼šç¡®ä¿æ¯ä¸ªè¯­è¨€ç›®å½•ä¸‹çš„ *.strings ä¸ Base.lproj ä¸€è‡´
    if run_doctor(cfg) != 0:
        print("âŒ sort ä¸­æ­¢ï¼šdoctor æœªé€šè¿‡")
        return

    try:
        created_dirs, created_files = ensure_strings_files_integrity(cfg)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    if created_dirs or created_files:
        print(f"âœ… å®Œæ•´æ€§ä¿®å¤ï¼šåˆ›å»ºç›®å½• {created_dirs} ä¸ªï¼Œåˆ›å»º .strings æ–‡ä»¶ {created_files} ä¸ª")
    else:
        print("âœ… å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡ï¼šå„è¯­è¨€ *.strings æ–‡ä»¶é›†ä¸ Base ä¸€è‡´")

    # é‡å¤å­—æ®µæ£€æŸ¥ï¼ˆè¯­è¨€ + listï¼‰ï¼Œç„¶åè®©ä½ å†³å®šç­–ç•¥
    dup_report = scan_duplicate_keys(cfg)
    policy = _resolve_duplicate_policy(cfg, dup_report)
    if policy == "cancel":
        print("âŒ sort å·²å–æ¶ˆï¼ˆæœªåšä»»ä½•ä¿®æ”¹ï¼‰")
        return

    # å†—ä½™å­—æ®µæ£€æŸ¥ï¼ˆBase ä¸­æ²¡æœ‰ï¼Œä½†å…¶ä»–è¯­è¨€æœ‰ï¼‰
    try:
        base_keys_map = _base_keys_by_file(cfg)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    redundant_report = scan_redundant_keys(cfg, base_keys_map)
    redundant_policy = _resolve_redundant_policy(cfg, redundant_report)
    if redundant_policy == "cancel":
        print("âŒ sort å·²å–æ¶ˆï¼ˆæœªåšä»»ä½•ä¿®æ”¹ï¼‰")
        return

    # 1) Base.lprojï¼šä¿ç•™æ³¨é‡Šï¼›æ³¨é‡Šåœ¨å­—æ®µä¸Šæ–¹ï¼›æŒ‰ key æ’åºå¹¶æŒ‰å‰ç¼€åˆ†ç»„
    try:
        base_changed = sort_base_strings_files(cfg, duplicate_policy=policy)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    # 2) å…¶ä»–è¯­è¨€ï¼šä»…æŒ‰ key æ’åºï¼ˆä¸åšå‰ç¼€åˆ†ç»„ï¼‰
    try:
        other_changed = sort_other_locale_strings_files(cfg, duplicate_policy=policy, base_keys_map=base_keys_map, redundant_policy=redundant_policy)
    except ConfigError as e:
        print(f"âŒ sort ä¸­æ­¢ï¼š{e}")
        return

    if base_changed:
        print(f"âœ… Base.lproj æ’åºå®Œæˆï¼šæ›´æ–° {base_changed} ä¸ª .strings æ–‡ä»¶")
    else:
        print("âœ… Base.lproj å·²æ˜¯æœ‰åºçŠ¶æ€ï¼šæ— éœ€æ”¹åŠ¨")

    if other_changed:
        print(f"âœ… å…¶ä»–è¯­è¨€æ’åºå®Œæˆï¼šæ›´æ–° {other_changed} ä¸ª .strings æ–‡ä»¶")
    else:
        print("âœ… å…¶ä»–è¯­è¨€å·²æ˜¯æœ‰åºçŠ¶æ€ï¼šæ— éœ€æ”¹åŠ¨")

